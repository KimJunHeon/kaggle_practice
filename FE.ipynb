{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "window 돌면서 feature engineering\n",
    "\n",
    "train시에는 첫달 버리고 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import joblib\n",
    "import datetime\n",
    "import itertools\n",
    "import os.path as path\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "font_dirs = ['/home/workspace/user-workspace/font']\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    "\n",
    "for font_file in font_files:\n",
    "    font_manager.fontManager.addfont(font_file)\n",
    "    \n",
    "plt.rcParams['font.family'] = 'NanumGothic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/home/workspace/user-workspace/slim_train.parquet'\n",
    "test_path ='/home/workspace/user-workspace/slim_test.parquet'\n",
    "encoder = '/home/workspace/user-workspace/cat_encoder.json'\n",
    "decoder = '/home/workspace/user-workspace/inverse_cat_encoder.json'\n",
    "data_dir = '/home/workspace/user-workspace/junheon/data/task150/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_ratio = 1\n",
    "bagging_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(train_path).reset_index().rename(columns={\"index\": \"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet(test_path).reset_index().rename(columns={\"index\": \"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/workspace/user-workspace/cat_encoder.json\") as json_file:\n",
    "    decoder = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7866548, 42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Indices \n",
    "# (7월, ACUM_RCPT_AMT<1 빼고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 04:56:56.126504\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REQ_DD', \"ACUM_RCPT_AMT\", 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-b592686c83d0>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['datetime'] = pd.to_datetime(df_train['REQ_DD'], format='%Y%m%d')\n",
      "<ipython-input-11-b592686c83d0>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['month'] = df_train['datetime'].dt.month.astype(\"uint8\")\n"
     ]
    }
   ],
   "source": [
    "df_train['datetime'] = pd.to_datetime(df_train['REQ_DD'], format='%Y%m%d')\n",
    "df_train['month'] = df_train['datetime'].dt.month.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = df_train[(df_train['target']==1)&(df_train['month']!=7)&(df_train['ACUM_RCPT_AMT']>=1)]\n",
    "negative = df_train[(df_train['target']==0)&(df_train['month']!=7)&(df_train['ACUM_RCPT_AMT']>=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58665\n",
      "5653077\n"
     ]
    }
   ],
   "source": [
    "print(len(positive))\n",
    "print(len(negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    negative_sample = negative.sample(n=(len(positive)*negative_ratio), random_state=seed)\n",
    "    id_list = positive.id.tolist() + negative_sample.id.tolist()\n",
    "    joblib.dump(id_list, f\"{data_dir}indices_{seed}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 10:11:09.701543\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"basic_feature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_features = {\n",
    "    \"id\": \"uint32\",\n",
    "    \"AC_PAY_AMT\": \"uint32\",\n",
    "    \"AGE\": \"uint8\",\n",
    "    \"SMS_RE_SND_CNT\": \"uint8\",\n",
    "    \"ACUM_RCPT_AMT\": \"int32\",\n",
    "    \"MAX_NPAY_CNT_24M\": \"uint8\",\n",
    "    \"TRD_CNT_6M\": \"uint8\",\n",
    "    \"REAL_TRD_CNT_6M\": \"uint8\",\n",
    "    \"NPAY_CNT_24M\": \"uint8\",\n",
    "    \"NPAY_CNT_12MNTS\": \"uint8\",\n",
    "    \"MM_LMT_AMT\": \"float32\",\n",
    "    \"REMD_LMT_AMT\": \"float32\",\n",
    "    \"NPAY_AMT_24M\": \"float32\",\n",
    "    \"NIGHT_TRD_RT_6M\": \"float32\",\n",
    "    \"AVG_AMT_6M\": \"float32\",\n",
    "    \"MAX_LMT_3M_RT\": \"float32\",\n",
    "    \"NPAY_AMT_60M\": \"float32\",\n",
    "    \"SUB_IP_A\": \"uint16\",\n",
    "    \"SUB_IP_B\": \"uint16\",\n",
    "    \"SUB_IP_C\": \"uint16\",\n",
    "    \"SUB_IP_D\": \"uint16\",\n",
    "    \"CP_CD\": \"uint32\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_features = [\n",
    "    \"COMMC_CLF\", \"NPAY_YN\", \"PAY_MTHD_CD\", \"ARS_AUTHTI_YN\", \"GNDR\", \"FOREI_YN\",  \"AUTHTI_CLF_FLG\", \n",
    "    \"SVC_CLF_NM\", \"CP_M_CLF_NM\", \"CP_S_CLF_NM\" \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[list(basic_features.keys()) + category_features + [\"target\"]].astype(basic_features)\n",
    "df_test = test_df[list(basic_features.keys()) + category_features].astype(basic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train['NPAY_YN'] = [-99 if x==2 else x for x in df_train['NPAY_YN']]\n",
    "df_test['NPAY_YN'] = [-99 if x==2 else x for x in df_test['NPAY_YN']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train['PAY_MTHD_CD'] = [-99 if x==0 else x for x in df_train['PAY_MTHD_CD']]\n",
    "df_test['PAY_MTHD_CD'] = [-99 if x==0 else x for x in df_test['PAY_MTHD_CD']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train['MM_LMT_AMT'] = [1000000 if np.isnan(x) else x for x in df_train['MM_LMT_AMT']]\n",
    "df_test['MM_LMT_AMT'] = [1000000 if np.isnan(x) else x for x in df_test['MM_LMT_AMT']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train['ARS_AUTHTI_YN'] = [-99 if x==2 else x for x in df_train['ARS_AUTHTI_YN']]\n",
    "df_test['ARS_AUTHTI_YN'] = [-99 if x==2 else x for x in df_test['ARS_AUTHTI_YN']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train['CP_M_CLF_NM'] = [-99 if x==5 else x for x in df_train['CP_M_CLF_NM']]\n",
    "df_test['CP_M_CLF_NM'] = [-99 if x==5 else x for x in df_test['CP_M_CLF_NM']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train['CP_S_CLF_NM'] = [-99 if x==34 else x for x in df_train['CP_S_CLF_NM']]\n",
    "df_test['CP_S_CLF_NM'] = [-99 if x==34 else x for x in df_test['CP_S_CLF_NM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# day, weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 04:58:28.225266\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"day_weekday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REQ_DD']].astype({\"REQ_DD\": \"str\"})\n",
    "df_test = test_df[['id', 'REQ_DD']].astype({\"REQ_DD\": \"str\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['datetime'] = pd.to_datetime(df_train['REQ_DD'], format='%Y%m%d')\n",
    "df_test['datetime'] = pd.to_datetime(df_test['REQ_DD'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['day'] = df_train['datetime'].dt.day.astype(\"uint8\")\n",
    "df_train['weekday'] = df_train['datetime'].dt.weekday.astype('uint8')\n",
    "df_train['month'] = df_train['datetime'].dt.month.astype(\"uint8\")\n",
    "df_test['day'] = df_test['datetime'].dt.day.astype(\"uint8\")\n",
    "df_test['weekday'] = df_test['datetime'].dt.weekday.astype('uint8')\n",
    "df_test['month'] = df_test['datetime'].dt.month.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'day', 'weekday', 'month']]\n",
    "df_test = df_test[['id', 'day', 'weekday', 'month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-14 04:58:45.801035\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUB IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 04:58:45.806186\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"sub_ip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_features = {\n",
    "    \"id\": \"uint32\",\n",
    "    \"SUB_IP_A\": \"uint16\",\n",
    "    \"SUB_IP_B\": \"uint16\",\n",
    "    \"SUB_IP_C\": \"uint16\",\n",
    "    \"SUB_IP_D\": \"uint16\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[list(ip_features.keys())].astype(ip_features)\n",
    "df_test = test_df[list(ip_features.keys())].astype(ip_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-14 04:58:47.701355\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 거래금액 (AC_PAY_AMT) 49000원, 11000원\n",
    "\n",
    "49000원: 롤 rp 충전 최고 금액\n",
    "\n",
    "11000원: 아프리카 별풍선 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 04:58:47.706274\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"pay_amt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'AC_PAY_AMT']]\n",
    "df_test = test_df[['id', 'AC_PAY_AMT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-96c0763cc145>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_49900_PAY_AMT\"] = (df_train['AC_PAY_AMT']==49900)\n",
      "<ipython-input-42-96c0763cc145>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_11000_PAY_AMT\"] = (df_train['AC_PAY_AMT']==11000)\n",
      "<ipython-input-42-96c0763cc145>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_11000s_PAY_AMT\"] = (df_train['AC_PAY_AMT']%11000==0)\n",
      "<ipython-input-42-96c0763cc145>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_49900_PAY_AMT\"] = (df_test['AC_PAY_AMT']==49900)\n",
      "<ipython-input-42-96c0763cc145>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_11000_PAY_AMT\"] = (df_test['AC_PAY_AMT']==11000)\n",
      "<ipython-input-42-96c0763cc145>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_11000s_PAY_AMT\"] = (df_test['AC_PAY_AMT']%11000==0)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"is_49900_PAY_AMT\"] = (df_train['AC_PAY_AMT']==49900)\n",
    "df_train[\"is_11000_PAY_AMT\"] = (df_train['AC_PAY_AMT']==11000)\n",
    "df_train[\"is_11000s_PAY_AMT\"] = (df_train['AC_PAY_AMT']%11000==0)\n",
    "df_test[\"is_49900_PAY_AMT\"] = (df_test['AC_PAY_AMT']==49900)\n",
    "df_test[\"is_11000_PAY_AMT\"] = (df_test['AC_PAY_AMT']==11000)\n",
    "df_test[\"is_11000s_PAY_AMT\"] = (df_test['AC_PAY_AMT']%11000==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'is_49900_PAY_AMT', 'is_11000_PAY_AMT', 'is_11000s_PAY_AMT']]\n",
    "df_test = df_test[['id', 'is_49900_PAY_AMT', 'is_11000_PAY_AMT', 'is_11000s_PAY_AMT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-14 04:58:49.288392\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store(CP_CD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 04:58:53.144600\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"cp_count\"\n",
    "COLUMN = \"CP_CD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD']], test_df[['id', COLUMN, 'REQ_DD']]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})\n",
    "df_test = test_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window size: 3D\n",
      "window size: 10D\n",
      "window size: 30D\n"
     ]
    }
   ],
   "source": [
    "for window_size in WINDOW_SIZE:\n",
    "    print(f\"window size: {window_size}D\")\n",
    "    count = df.groupby([COLUMN, 'REQ_DD']).count().reset_index().rename(columns={\"id\": \"count\"})\n",
    "    count['datetime'] = pd.to_datetime(count['REQ_DD'], format='%Y%m%d')\n",
    "    count = count.drop(columns=['REQ_DD'])\n",
    "    cum_count = count.sort_values(by=['datetime']).groupby([COLUMN]).rolling(f\"{window_size+1}D\", min_periods=1, on='datetime')['count'].sum().reset_index().rename(columns={\"count\": f\"cum_{window_size}_count\"})\n",
    "    cum_count['REQ_DD'] = cum_count['datetime'].dt.strftime('%Y%m%d')\n",
    "    cum_count = cum_count.drop(columns=['datetime'])\n",
    "    count['REQ_DD'] = count['datetime'].dt.strftime('%Y%m%d')\n",
    "    count = count.drop(columns=['datetime'])\n",
    "    count = count.merge(cum_count, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "    count[f'{FILE_NAME}_{window_size}'] = count[f'cum_{window_size}_count'] - count['count']\n",
    "    count = count[[COLUMN, 'REQ_DD', f'{FILE_NAME}_{window_size}']].astype({'REQ_DD': 'int32'})\n",
    "    df_train = df_train.merge(count, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "    df_test = df_test.merge(count, on=[COLUMN, 'REQ_DD'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=['REQ_DD', COLUMN])\n",
    "df_test = df_test.drop(columns=['REQ_DD', COLUMN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-14 04:59:12.741398\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unique count: 'GODS_NM', 'PAYR_SEQ', 'PAYR_IP', 'MPHN_NO', 'COMMC_CLF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 05:09:01.159047\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"unique_count_wrt_cp\"\n",
    "COLUMN = \"CP_CD\"\n",
    "TARGET_LIST = [\"GODS_NM\", \"PAYR_SEQ\", \"PAYR_IP\", \"MPHN_NO\", \"COMMC_CLF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST], test_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST]], \n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: GODS_NM\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_GODS_NM_count_wrt_CP_CD_3 exists!\n",
      "window: 3, target: PAYR_SEQ\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_PAYR_SEQ_count_wrt_CP_CD_3 exists!\n",
      "window: 3, target: PAYR_IP\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_PAYR_IP_count_wrt_CP_CD_3 exists!\n",
      "window: 3, target: MPHN_NO\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_MPHN_NO_count_wrt_CP_CD_3 exists!\n",
      "window: 3, target: COMMC_CLF\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_COMMC_CLF_count_wrt_CP_CD_3 exists!\n",
      "window: 10, target: GODS_NM\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_GODS_NM_count_wrt_CP_CD_10 exists!\n",
      "window: 10, target: PAYR_SEQ\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_PAYR_SEQ_count_wrt_CP_CD_10 exists!\n",
      "window: 10, target: PAYR_IP\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_PAYR_IP_count_wrt_CP_CD_10 exists!\n",
      "window: 10, target: MPHN_NO\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_MPHN_NO_count_wrt_CP_CD_10 exists!\n",
      "window: 10, target: COMMC_CLF\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_COMMC_CLF_count_wrt_CP_CD_10 exists!\n",
      "window: 30, target: GODS_NM\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_GODS_NM_count_wrt_CP_CD_30 exists!\n",
      "window: 30, target: PAYR_SEQ\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_PAYR_SEQ_count_wrt_CP_CD_30 exists!\n",
      "window: 30, target: PAYR_IP\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_PAYR_IP_count_wrt_CP_CD_30 exists!\n",
      "window: 30, target: MPHN_NO\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_MPHN_NO_count_wrt_CP_CD_30 exists!\n",
      "window: 30, target: COMMC_CLF\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_COMMC_CLF_count_wrt_CP_CD_30 exists!\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}unique_{target}_count_wrt_{COLUMN}_{window}\"\n",
    "        if path.exists(data_path):\n",
    "            print(f\"{data_path} exists!\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        temp_df = df[[COLUMN, target, 'REQ_DD']].drop_duplicates(subset=[COLUMN, target, 'REQ_DD']).sort_values(by=['REQ_DD'])\n",
    "        temp_df['datetime'] = pd.to_datetime(temp_df['REQ_DD'], format='%Y%m%d')\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "        \n",
    "        count_list = []\n",
    "        start_date = \"2019-07-01\"\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        while True:\n",
    "            end_date = start_date + datetime.timedelta(days=window)\n",
    "\n",
    "            if end_date > datetime.datetime.strptime(\"2019-12-31\", \"%Y-%m-%d\"):\n",
    "                break\n",
    "\n",
    "            count_df = temp_df[(temp_df['datetime']>=start_date)&(temp_df['datetime']<end_date)][[COLUMN, target]]\n",
    "            count_df = count_df.groupby([COLUMN]).agg(['nunique']).reset_index()\n",
    "            count_df.columns = [COLUMN, f'unique_{target}_wrt_{COLUMN}_{window}']\n",
    "            count_df['REQ_DD'] = datetime.datetime.strftime(end_date, \"%Y%m%d\")\n",
    "\n",
    "            count_list.append(count_df)\n",
    "            start_date = start_date + datetime.timedelta(days=1)\n",
    "        count_df = pd.concat(count_list, axis=0)\n",
    "        count_df.to_parquet(data_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})\n",
    "df_test = test_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: GODS_NM\n",
      "window: 3, target: PAYR_SEQ\n",
      "window: 3, target: PAYR_IP\n",
      "window: 3, target: MPHN_NO\n",
      "window: 3, target: COMMC_CLF\n",
      "window: 10, target: GODS_NM\n",
      "window: 10, target: PAYR_SEQ\n",
      "window: 10, target: PAYR_IP\n",
      "window: 10, target: MPHN_NO\n",
      "window: 10, target: COMMC_CLF\n",
      "window: 30, target: GODS_NM\n",
      "window: 30, target: PAYR_SEQ\n",
      "window: 30, target: PAYR_IP\n",
      "window: 30, target: MPHN_NO\n",
      "window: 30, target: COMMC_CLF\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}unique_{target}_count_wrt_{COLUMN}_{window}\"\n",
    "        \n",
    "        count_df = pd.read_parquet(data_path).astype({'REQ_DD': 'int32'})\n",
    "        df_train = df_train.merge(count_df, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "        df_test = df_test.merge(count_df, on=[COLUMN, 'REQ_DD'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].drop(columns=[COLUMN, 'REQ_DD']).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(columns=[COLUMN, 'REQ_DD']).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-12 07:43:24.644363\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## average: 'NPAY_AMT_24M', 'MAX_NPAY_CNT_24M', 'NIGHT_TRD_RT_6M' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 05:10:17.278208\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"average_wrt_cp\"\n",
    "COLUMN = \"CP_CD\"\n",
    "TARGET_LIST = [\"NPAY_AMT_24M\", \"MAX_NPAY_CNT_24M\", \"NIGHT_TRD_RT_6M\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST], test_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST]], \n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: NPAY_AMT_24M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_NPAY_AMT_24M_wrt_CP_CD_3 exists!\n",
      "window: 3, target: MAX_NPAY_CNT_24M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_MAX_NPAY_CNT_24M_wrt_CP_CD_3 exists!\n",
      "window: 3, target: NIGHT_TRD_RT_6M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_NIGHT_TRD_RT_6M_wrt_CP_CD_3 exists!\n",
      "window: 10, target: NPAY_AMT_24M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_NPAY_AMT_24M_wrt_CP_CD_10 exists!\n",
      "window: 10, target: MAX_NPAY_CNT_24M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_MAX_NPAY_CNT_24M_wrt_CP_CD_10 exists!\n",
      "window: 10, target: NIGHT_TRD_RT_6M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_NIGHT_TRD_RT_6M_wrt_CP_CD_10 exists!\n",
      "window: 30, target: NPAY_AMT_24M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_NPAY_AMT_24M_wrt_CP_CD_30 exists!\n",
      "window: 30, target: MAX_NPAY_CNT_24M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_MAX_NPAY_CNT_24M_wrt_CP_CD_30 exists!\n",
      "window: 30, target: NIGHT_TRD_RT_6M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_NIGHT_TRD_RT_6M_wrt_CP_CD_30 exists!\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}average_{target}_wrt_{COLUMN}_{window}\"\n",
    "        if path.exists(data_path):\n",
    "            print(f\"{data_path} exists!\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        temp_df = df[[COLUMN, target, 'REQ_DD']].sort_values(by=['REQ_DD'])\n",
    "        temp_df['datetime'] = pd.to_datetime(temp_df['REQ_DD'], format='%Y%m%d')\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "        \n",
    "        average_list = []\n",
    "        start_date = \"2019-07-01\"\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        while True:\n",
    "            end_date = start_date + datetime.timedelta(days=window)\n",
    "\n",
    "            if end_date > datetime.datetime.strptime(\"2019-12-31\", \"%Y-%m-%d\"):\n",
    "                break\n",
    "\n",
    "            average_df = temp_df[(temp_df['datetime']>=start_date)&(temp_df['datetime']<end_date)][[COLUMN, target]]\n",
    "            average_df = average_df.groupby([COLUMN]).mean().reset_index()\n",
    "            average_df.columns = [COLUMN, f'average_{target}_wrt_{COLUMN}_{window}']\n",
    "            average_df['REQ_DD'] = datetime.datetime.strftime(end_date, \"%Y%m%d\")\n",
    "\n",
    "            average_list.append(average_df)\n",
    "            start_date = start_date + datetime.timedelta(days=1)\n",
    "        average_df = pd.concat(average_list, axis=0)\n",
    "        average_df.to_parquet(data_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})\n",
    "df_test = test_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: NPAY_AMT_24M\n",
      "window: 3, target: MAX_NPAY_CNT_24M\n",
      "window: 3, target: NIGHT_TRD_RT_6M\n",
      "window: 10, target: NPAY_AMT_24M\n",
      "window: 10, target: MAX_NPAY_CNT_24M\n",
      "window: 10, target: NIGHT_TRD_RT_6M\n",
      "window: 30, target: NPAY_AMT_24M\n",
      "window: 30, target: MAX_NPAY_CNT_24M\n",
      "window: 30, target: NIGHT_TRD_RT_6M\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}average_{target}_wrt_{COLUMN}_{window}\"\n",
    "        \n",
    "        average_df = pd.read_parquet(data_path).astype({'REQ_DD': 'int32'})\n",
    "        df_train = df_train.merge(average_df, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "        df_test = df_test.merge(average_df, on=[COLUMN, 'REQ_DD'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].drop(columns=[COLUMN, 'REQ_DD']).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(columns=[COLUMN, 'REQ_DD']).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-12 11:28:20.351843\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proportion: 'ARS_AUTHTI_YN', 'NPAY_YN', 'GNDR', 'FOREI_YN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 각 feature 조건의 COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 08:02:12.314960\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"count_for_prop_wrt_cp\"\n",
    "COLUMN = \"CP_CD\"\n",
    "TARGET_LIST = [\"ARS_AUTHTI_YN\", \"NPAY_YN\", \"GNDR\", \"FOREI_YN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST], test_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST]], \n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: ARS_AUTHTI_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/ARS_AUTHTI_YN_count_for_prop_wrt_CP_CD_3 exists!\n",
      "window: 3, target: NPAY_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/NPAY_YN_count_for_prop_wrt_CP_CD_3 exists!\n",
      "window: 3, target: GNDR\n",
      "/home/workspace/user-workspace/junheon/data/task150/GNDR_count_for_prop_wrt_CP_CD_3 exists!\n",
      "window: 3, target: FOREI_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/FOREI_YN_count_for_prop_wrt_CP_CD_3 exists!\n",
      "window: 10, target: ARS_AUTHTI_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/ARS_AUTHTI_YN_count_for_prop_wrt_CP_CD_10 exists!\n",
      "window: 10, target: NPAY_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/NPAY_YN_count_for_prop_wrt_CP_CD_10 exists!\n",
      "window: 10, target: GNDR\n",
      "/home/workspace/user-workspace/junheon/data/task150/GNDR_count_for_prop_wrt_CP_CD_10 exists!\n",
      "window: 10, target: FOREI_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/FOREI_YN_count_for_prop_wrt_CP_CD_10 exists!\n",
      "window: 30, target: ARS_AUTHTI_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/ARS_AUTHTI_YN_count_for_prop_wrt_CP_CD_30 exists!\n",
      "window: 30, target: NPAY_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/NPAY_YN_count_for_prop_wrt_CP_CD_30 exists!\n",
      "window: 30, target: GNDR\n",
      "/home/workspace/user-workspace/junheon/data/task150/GNDR_count_for_prop_wrt_CP_CD_30 exists!\n",
      "window: 30, target: FOREI_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/FOREI_YN_count_for_prop_wrt_CP_CD_30 exists!\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}{target}_count_for_prop_wrt_{COLUMN}_{window}\"\n",
    "        if path.exists(data_path):\n",
    "            print(f\"{data_path} exists!\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        temp_df = df[[COLUMN, target, 'REQ_DD']].sort_values(by=['REQ_DD'])\n",
    "        temp_df['datetime'] = pd.to_datetime(temp_df['REQ_DD'], format='%Y%m%d')\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "        \n",
    "        count_list = []\n",
    "        start_date = \"2019-07-01\"\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        while True:\n",
    "            end_date = start_date + datetime.timedelta(days=window)\n",
    "\n",
    "            if end_date > datetime.datetime.strptime(\"2019-12-31\", \"%Y-%m-%d\"):\n",
    "                break\n",
    "\n",
    "            count_df = temp_df[(temp_df['datetime']>=start_date)&(temp_df['datetime']<end_date)&(temp_df[target]==1)][[COLUMN, target]]\n",
    "            count_df = count_df.groupby([COLUMN]).count().reset_index()\n",
    "            count_df.columns = [COLUMN, f'{target}_count_for_prop_wrt_{COLUMN}_{window}']\n",
    "            count_df['REQ_DD'] = datetime.datetime.strftime(end_date, \"%Y%m%d\")\n",
    "\n",
    "            count_list.append(count_df)\n",
    "            start_date = start_date + datetime.timedelta(days=1)\n",
    "        count_df = pd.concat(count_list, axis=0)\n",
    "        count_df.to_parquet(data_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})\n",
    "df_test = test_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: ARS_AUTHTI_YN\n",
      "window: 3, target: NPAY_YN\n",
      "window: 3, target: GNDR\n",
      "window: 3, target: FOREI_YN\n",
      "window: 10, target: ARS_AUTHTI_YN\n",
      "window: 10, target: NPAY_YN\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-8679c1b777fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mcount_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'REQ_DD'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int32'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCOLUMN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'REQ_DD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCOLUMN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'REQ_DD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   7944\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7946\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m   7947\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7948\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     )\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mrindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mright_indexer\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         result_data = concatenate_block_managers(\n\u001b[0m\u001b[1;32m    678\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mllabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             b = make_block(\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0m_concatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36m_concatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;31m# in JoinUnit.get_reindexed_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                     \u001b[0mconcat_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0mconcat_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}{target}_count_for_prop_wrt_{COLUMN}_{window}\"\n",
    "        \n",
    "        count_df = pd.read_parquet(data_path).astype({'REQ_DD': 'int32'})\n",
    "        df_train = df_train.merge(count_df, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "        df_test = df_test.merge(count_df, on=[COLUMN, 'REQ_DD'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].drop(columns=[COLUMN, 'REQ_DD']).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.fillna(0).drop(columns=[COLUMN, 'REQ_DD']).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-12 11:44:24.141346\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count와 count_for_prop 불러와서 비율 구하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FILE_NAME = \"cp_count\"\n",
    "RESULT_FILE_NAME = \"prop_wrt_cp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    count_df = pd.read_parquet(f\"{data_dir}{BASE_FILE_NAME}_{seed}.parquet\")\n",
    "    count_for_prop_df = pd.read_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")\n",
    "    COLUMNS = [\"id\"]\n",
    "    for window in WINDOW_SIZE:\n",
    "        for target in TARGET_LIST:\n",
    "            count_df[f\"prop_{target}_wrt_{COLUMN}_{window}\"] = \\\n",
    "                count_for_prop_df[f'{target}_count_for_prop_wrt_{COLUMN}_{window}'] / \\\n",
    "                count_df[f'{BASE_FILE_NAME}_{window}']\n",
    "            COLUMNS.append(f\"prop_{target}_wrt_{COLUMN}_{window}\")\n",
    "    count_df[COLUMNS].to_parquet(f\"{data_dir}{RESULT_FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = pd.read_parquet(f\"{data_dir}{BASE_FILE_NAME}_test.parquet\")\n",
    "count_for_prop_df = pd.read_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")\n",
    "COLUMNS = [\"id\"]\n",
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        count_df[f\"prop_{target}_wrt_{COLUMN}_{window}\"] = \\\n",
    "            count_for_prop_df[f'{target}_count_for_prop_wrt_{COLUMN}_{window}'] / \\\n",
    "            count_df[f'{BASE_FILE_NAME}_{window}']\n",
    "        COLUMNS.append(f\"prop_{target}_wrt_{COLUMN}_{window}\")\n",
    "count_df[COLUMNS].to_parquet(f\"{data_dir}{RESULT_FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ratio: unique('PAYR_IP')/unique('MPHN_NO'), unique('GODS_NM')/unique('PAYR_SEQ'), unique('PAYR_IP')/unique('PAYR_SEQ'), unique('MPHN_NO')/unique('PAYR_SEQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 08:03:15.185034\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_FILE_NAME = \"unique_count_wrt_cp\"\n",
    "FILE_NAME = \"unique_divide_wrt_cp\"\n",
    "COLUMN = \"CP_CD\"\n",
    "COLUMNS = []\n",
    "TARGET_LIST = [\"GODS_NM\", \"PAYR_SEQ\", \"PAYR_IP\", \"MPHN_NO\"]\n",
    "NEW_TARGET_LIST = [(\"PAYR_IP\", \"MPHN_NO\"), (\"GODS_NM\", \"PAYR_SEQ\"), (\"PAYR_IP\", \"PAYR_SEQ\"), (\"MPHN_NO\", \"PAYR_SEQ\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        COLUMNS.append(f'unique_{target}_wrt_{COLUMN}_{window}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    df_train = pd.read_parquet(f\"{data_dir}{LOAD_FILE_NAME}_{seed}.parquet\")[[\"id\"] + COLUMNS]\n",
    "    new_columns = []\n",
    "    for window in WINDOW_SIZE:\n",
    "        for new_target in NEW_TARGET_LIST:\n",
    "            new_column = f\"unique_{new_target[0]}_by_{new_target[1]}_wrt_{COLUMN}_{window}\"\n",
    "            df_train[new_column] = df_train[f\"unique_{new_target[0]}_wrt_{COLUMN}_{window}\"] / df_train[f\"unique_{new_target[1]}_wrt_{COLUMN}_{window}\"]\n",
    "            new_columns.append(new_column)\n",
    "    df_train = df_train[[\"id\"] + new_columns].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet(f\"{data_dir}{LOAD_FILE_NAME}_test.parquet\")[[\"id\"] + COLUMNS]\n",
    "\n",
    "new_columns = []\n",
    "for window in WINDOW_SIZE:\n",
    "    for new_target in NEW_TARGET_LIST:\n",
    "        new_column = f\"unique_{new_target[0]}_by_{new_target[1]}_wrt_{COLUMN}_{window}\"\n",
    "        df_test[new_column] = df_test[f\"unique_{new_target[0]}_wrt_{COLUMN}_{window}\"] / df_test[f\"unique_{new_target[1]}_wrt_{COLUMN}_{window}\"]\n",
    "        new_columns.append(new_column)\n",
    "df_test = df_test[[\"id\"] + new_columns].to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-12 12:16:45.475957\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item(GODS_NM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 05:21:47.850967\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"gods_count\"\n",
    "COLUMN = \"GODS_NM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD']], test_df[['id', COLUMN, 'REQ_DD']]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})\n",
    "df_test = test_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window size: 3D\n",
      "window size: 10D\n",
      "window size: 30D\n"
     ]
    }
   ],
   "source": [
    "for window_size in WINDOW_SIZE:\n",
    "    print(f\"window size: {window_size}D\")\n",
    "    count = df.groupby([COLUMN, 'REQ_DD']).count().reset_index().rename(columns={\"id\": \"count\"})\n",
    "    count['datetime'] = pd.to_datetime(count['REQ_DD'], format='%Y%m%d')\n",
    "    count = count.drop(columns=['REQ_DD'])\n",
    "    cum_count = count.sort_values(by=['datetime']).groupby([COLUMN]).rolling(f\"{window_size+1}D\", min_periods=1, on='datetime')['count'].sum().reset_index().rename(columns={\"count\": f\"cum_{window_size}_count\"})\n",
    "    cum_count['REQ_DD'] = cum_count['datetime'].dt.strftime('%Y%m%d')\n",
    "    cum_count = cum_count.drop(columns=['datetime'])\n",
    "    count['REQ_DD'] = count['datetime'].dt.strftime('%Y%m%d')\n",
    "    count = count.drop(columns=['datetime'])\n",
    "    count = count.merge(cum_count, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "    count[f'{FILE_NAME}_{window_size}'] = count[f'cum_{window_size}_count'] - count['count']\n",
    "    count = count[[COLUMN, 'REQ_DD', f'{FILE_NAME}_{window_size}']].astype({'REQ_DD': 'int32'})\n",
    "    df_train = df_train.merge(count, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "    df_test = df_test.merge(count, on=[COLUMN, 'REQ_DD'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=['REQ_DD', COLUMN])\n",
    "df_test = df_test.drop(columns=['REQ_DD', COLUMN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-12 07:49:06.811148\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unique count: 'CP_CD', 'PAYR_SEQ', 'PAYR_IP', 'MPHN_NO', 'COMMC_CLF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 05:11:29.312257\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"unique_count_wrt_gods\"\n",
    "COLUMN = \"GODS_NM\"\n",
    "TARGET_LIST = [\"CP_CD\", \"PAYR_SEQ\", \"PAYR_IP\", \"MPHN_NO\", \"COMMC_CLF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST], test_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST]], \n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: CP_CD\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_CP_CD_count_wrt_GODS_NM_3 exists!\n",
      "window: 3, target: PAYR_SEQ\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_PAYR_SEQ_count_wrt_GODS_NM_3 exists!\n",
      "window: 3, target: PAYR_IP\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_PAYR_IP_count_wrt_GODS_NM_3 exists!\n",
      "window: 3, target: MPHN_NO\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_MPHN_NO_count_wrt_GODS_NM_3 exists!\n",
      "window: 3, target: COMMC_CLF\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_COMMC_CLF_count_wrt_GODS_NM_3 exists!\n",
      "window: 10, target: CP_CD\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_CP_CD_count_wrt_GODS_NM_10 exists!\n",
      "window: 10, target: PAYR_SEQ\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_PAYR_SEQ_count_wrt_GODS_NM_10 exists!\n",
      "window: 10, target: PAYR_IP\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_PAYR_IP_count_wrt_GODS_NM_10 exists!\n",
      "window: 10, target: MPHN_NO\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_MPHN_NO_count_wrt_GODS_NM_10 exists!\n",
      "window: 10, target: COMMC_CLF\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_COMMC_CLF_count_wrt_GODS_NM_10 exists!\n",
      "window: 30, target: CP_CD\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_CP_CD_count_wrt_GODS_NM_30 exists!\n",
      "window: 30, target: PAYR_SEQ\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_PAYR_SEQ_count_wrt_GODS_NM_30 exists!\n",
      "window: 30, target: PAYR_IP\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_PAYR_IP_count_wrt_GODS_NM_30 exists!\n",
      "window: 30, target: MPHN_NO\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_MPHN_NO_count_wrt_GODS_NM_30 exists!\n",
      "window: 30, target: COMMC_CLF\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_COMMC_CLF_count_wrt_GODS_NM_30 exists!\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}unique_{target}_count_wrt_{COLUMN}_{window}\"\n",
    "        if path.exists(data_path):\n",
    "            print(f\"{data_path} exists!\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        temp_df = df[[COLUMN, target, 'REQ_DD']].drop_duplicates(subset=[COLUMN, target, 'REQ_DD']).sort_values(by=['REQ_DD'])\n",
    "        temp_df['datetime'] = pd.to_datetime(temp_df['REQ_DD'], format='%Y%m%d')\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "        \n",
    "        count_list = []\n",
    "        start_date = \"2019-07-01\"\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        while True:\n",
    "            end_date = start_date + datetime.timedelta(days=window)\n",
    "\n",
    "            if end_date > datetime.datetime.strptime(\"2019-12-31\", \"%Y-%m-%d\"):\n",
    "                break\n",
    "\n",
    "            count_df = temp_df[(temp_df['datetime']>=start_date)&(temp_df['datetime']<end_date)][[COLUMN, target]]\n",
    "            count_df = count_df.groupby([COLUMN]).agg(['nunique']).reset_index()\n",
    "            count_df.columns = [COLUMN, f'unique_{target}_wrt_{COLUMN}_{window}']\n",
    "            count_df['REQ_DD'] = datetime.datetime.strftime(end_date, \"%Y%m%d\")\n",
    "\n",
    "            count_list.append(count_df)\n",
    "            start_date = start_date + datetime.timedelta(days=1)\n",
    "        count_df = pd.concat(count_list, axis=0)\n",
    "        count_df.to_parquet(data_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})\n",
    "df_test = test_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: CP_CD\n",
      "window: 3, target: PAYR_SEQ\n",
      "window: 3, target: PAYR_IP\n",
      "window: 3, target: MPHN_NO\n",
      "window: 3, target: COMMC_CLF\n",
      "window: 10, target: CP_CD\n",
      "window: 10, target: PAYR_SEQ\n",
      "window: 10, target: PAYR_IP\n",
      "window: 10, target: MPHN_NO\n",
      "window: 10, target: COMMC_CLF\n",
      "window: 30, target: CP_CD\n",
      "window: 30, target: PAYR_SEQ\n",
      "window: 30, target: PAYR_IP\n",
      "window: 30, target: MPHN_NO\n",
      "window: 30, target: COMMC_CLF\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}unique_{target}_count_wrt_{COLUMN}_{window}\"\n",
    "        \n",
    "        count_df = pd.read_parquet(data_path).astype({'REQ_DD': 'int32'})\n",
    "        df_train = df_train.merge(count_df, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "        df_test = df_test.merge(count_df, on=[COLUMN, 'REQ_DD'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].drop(columns=[COLUMN, 'REQ_DD']).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(columns=[COLUMN, 'REQ_DD']).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-12 08:06:55.861451\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## average: 'NPAY_AMT_24M', 'MAX_NPAY_CNT_24M', 'NIGHT_TRD_RT_6M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 05:16:02.921330\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"average_wrt_gods\"\n",
    "COLUMN = \"GODS_NM\"\n",
    "TARGET_LIST = [\"NPAY_AMT_24M\", \"MAX_NPAY_CNT_24M\", \"NIGHT_TRD_RT_6M\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST], test_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST]], \n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: NPAY_AMT_24M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_NPAY_AMT_24M_wrt_GODS_NM_3 exists!\n",
      "window: 3, target: MAX_NPAY_CNT_24M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_MAX_NPAY_CNT_24M_wrt_GODS_NM_3 exists!\n",
      "window: 3, target: NIGHT_TRD_RT_6M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_NIGHT_TRD_RT_6M_wrt_GODS_NM_3 exists!\n",
      "window: 10, target: NPAY_AMT_24M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_NPAY_AMT_24M_wrt_GODS_NM_10 exists!\n",
      "window: 10, target: MAX_NPAY_CNT_24M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_MAX_NPAY_CNT_24M_wrt_GODS_NM_10 exists!\n",
      "window: 10, target: NIGHT_TRD_RT_6M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_NIGHT_TRD_RT_6M_wrt_GODS_NM_10 exists!\n",
      "window: 30, target: NPAY_AMT_24M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_NPAY_AMT_24M_wrt_GODS_NM_30 exists!\n",
      "window: 30, target: MAX_NPAY_CNT_24M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_MAX_NPAY_CNT_24M_wrt_GODS_NM_30 exists!\n",
      "window: 30, target: NIGHT_TRD_RT_6M\n",
      "/home/workspace/user-workspace/junheon/data/task150/average_NIGHT_TRD_RT_6M_wrt_GODS_NM_30 exists!\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}average_{target}_wrt_{COLUMN}_{window}\"\n",
    "        if path.exists(data_path):\n",
    "            print(f\"{data_path} exists!\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        temp_df = df[[COLUMN, target, 'REQ_DD']].sort_values(by=['REQ_DD'])\n",
    "        temp_df['datetime'] = pd.to_datetime(temp_df['REQ_DD'], format='%Y%m%d')\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "        \n",
    "        average_list = []\n",
    "        start_date = \"2019-07-01\"\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        while True:\n",
    "            end_date = start_date + datetime.timedelta(days=window)\n",
    "\n",
    "            if end_date > datetime.datetime.strptime(\"2019-12-31\", \"%Y-%m-%d\"):\n",
    "                break\n",
    "\n",
    "            average_df = temp_df[(temp_df['datetime']>=start_date)&(temp_df['datetime']<end_date)][[COLUMN, target]]\n",
    "            average_df = average_df.groupby([COLUMN]).mean().reset_index()\n",
    "            average_df.columns = [COLUMN, f'average_{target}_wrt_{COLUMN}_{window}']\n",
    "            average_df['REQ_DD'] = datetime.datetime.strftime(end_date, \"%Y%m%d\")\n",
    "\n",
    "            average_list.append(average_df)\n",
    "            start_date = start_date + datetime.timedelta(days=1)\n",
    "        average_df = pd.concat(average_list, axis=0)\n",
    "        average_df.to_parquet(data_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})\n",
    "df_test = test_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: NPAY_AMT_24M\n",
      "window: 3, target: MAX_NPAY_CNT_24M\n",
      "window: 3, target: NIGHT_TRD_RT_6M\n",
      "window: 10, target: NPAY_AMT_24M\n",
      "window: 10, target: MAX_NPAY_CNT_24M\n",
      "window: 10, target: NIGHT_TRD_RT_6M\n",
      "window: 30, target: NPAY_AMT_24M\n",
      "window: 30, target: MAX_NPAY_CNT_24M\n",
      "window: 30, target: NIGHT_TRD_RT_6M\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}average_{target}_wrt_{COLUMN}_{window}\"\n",
    "        \n",
    "        average_df = pd.read_parquet(data_path).astype({'REQ_DD': 'int32'})\n",
    "        df_train = df_train.merge(average_df, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "        df_test = df_test.merge(average_df, on=[COLUMN, 'REQ_DD'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].drop(columns=[COLUMN, 'REQ_DD']).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(columns=[COLUMN, 'REQ_DD']).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-12 11:37:17.865382\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proposition: 'ARS_AUTHTI_YN', 'NPAY_YN', 'GNDR', 'FOREI_YN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 각 feature 조건의 COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 05:27:11.757375\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"count_for_prop_wrt_gods\"\n",
    "COLUMN = \"GODS_NM\"\n",
    "TARGET_LIST = [\"ARS_AUTHTI_YN\", \"NPAY_YN\", \"GNDR\", \"FOREI_YN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST], test_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST]], \n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: ARS_AUTHTI_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/ARS_AUTHTI_YN_count_for_prop_wrt_GODS_NM_3 exists!\n",
      "window: 3, target: NPAY_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/NPAY_YN_count_for_prop_wrt_GODS_NM_3 exists!\n",
      "window: 3, target: GNDR\n",
      "/home/workspace/user-workspace/junheon/data/task150/GNDR_count_for_prop_wrt_GODS_NM_3 exists!\n",
      "window: 3, target: FOREI_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/FOREI_YN_count_for_prop_wrt_GODS_NM_3 exists!\n",
      "window: 10, target: ARS_AUTHTI_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/ARS_AUTHTI_YN_count_for_prop_wrt_GODS_NM_10 exists!\n",
      "window: 10, target: NPAY_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/NPAY_YN_count_for_prop_wrt_GODS_NM_10 exists!\n",
      "window: 10, target: GNDR\n",
      "/home/workspace/user-workspace/junheon/data/task150/GNDR_count_for_prop_wrt_GODS_NM_10 exists!\n",
      "window: 10, target: FOREI_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/FOREI_YN_count_for_prop_wrt_GODS_NM_10 exists!\n",
      "window: 30, target: ARS_AUTHTI_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/ARS_AUTHTI_YN_count_for_prop_wrt_GODS_NM_30 exists!\n",
      "window: 30, target: NPAY_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/NPAY_YN_count_for_prop_wrt_GODS_NM_30 exists!\n",
      "window: 30, target: GNDR\n",
      "/home/workspace/user-workspace/junheon/data/task150/GNDR_count_for_prop_wrt_GODS_NM_30 exists!\n",
      "window: 30, target: FOREI_YN\n",
      "/home/workspace/user-workspace/junheon/data/task150/FOREI_YN_count_for_prop_wrt_GODS_NM_30 exists!\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}{target}_count_for_prop_wrt_{COLUMN}_{window}\"\n",
    "        if path.exists(data_path):\n",
    "            print(f\"{data_path} exists!\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        temp_df = df[[COLUMN, target, 'REQ_DD']].sort_values(by=['REQ_DD'])\n",
    "        temp_df['datetime'] = pd.to_datetime(temp_df['REQ_DD'], format='%Y%m%d')\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "        \n",
    "        count_list = []\n",
    "        start_date = \"2019-07-01\"\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        while True:\n",
    "            end_date = start_date + datetime.timedelta(days=window)\n",
    "\n",
    "            if end_date > datetime.datetime.strptime(\"2019-12-31\", \"%Y-%m-%d\"):\n",
    "                break\n",
    "\n",
    "            count_df = temp_df[(temp_df['datetime']>=start_date)&(temp_df['datetime']<end_date)&(temp_df[target]==1)][[COLUMN, target]]\n",
    "            count_df = count_df.groupby([COLUMN]).count().reset_index()\n",
    "            count_df.columns = [COLUMN, f'{target}_count_for_prop_wrt_{COLUMN}_{window}']\n",
    "            count_df['REQ_DD'] = datetime.datetime.strftime(end_date, \"%Y%m%d\")\n",
    "\n",
    "            count_list.append(count_df)\n",
    "            start_date = start_date + datetime.timedelta(days=1)\n",
    "        count_df = pd.concat(count_list, axis=0)\n",
    "        count_df.to_parquet(data_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})\n",
    "df_test = test_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: ARS_AUTHTI_YN\n",
      "window: 3, target: NPAY_YN\n",
      "window: 3, target: GNDR\n",
      "window: 3, target: FOREI_YN\n",
      "window: 10, target: ARS_AUTHTI_YN\n",
      "window: 10, target: NPAY_YN\n",
      "window: 10, target: GNDR\n",
      "window: 10, target: FOREI_YN\n",
      "window: 30, target: ARS_AUTHTI_YN\n",
      "window: 30, target: NPAY_YN\n",
      "window: 30, target: GNDR\n",
      "window: 30, target: FOREI_YN\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}{target}_count_for_prop_wrt_{COLUMN}_{window}\"\n",
    "        \n",
    "        count_df = pd.read_parquet(data_path).astype({'REQ_DD': 'int32'})\n",
    "        df_train = df_train.merge(count_df, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "        df_test = df_test.merge(count_df, on=[COLUMN, 'REQ_DD'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].drop(columns=[COLUMN, 'REQ_DD']).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.fillna(0).drop(columns=[COLUMN, 'REQ_DD']).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-12 11:57:13.557496\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count와 count_for_prop 불러와서 비율 구하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FILE_NAME = \"gods_count\"\n",
    "RESULT_FILE_NAME = \"prop_wrt_gods\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    count_df = pd.read_parquet(f\"{data_dir}{BASE_FILE_NAME}_{seed}.parquet\")\n",
    "    count_for_prop_df = pd.read_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")\n",
    "    COLUMNS = [\"id\"]\n",
    "    for window in WINDOW_SIZE:\n",
    "        for target in TARGET_LIST:\n",
    "            count_df[f\"prop_{target}_wrt_{COLUMN}_{window}\"] = \\\n",
    "                count_for_prop_df[f'{target}_count_for_prop_wrt_{COLUMN}_{window}'] / \\\n",
    "                count_df[f'{BASE_FILE_NAME}_{window}']\n",
    "            COLUMNS.append(f\"prop_{target}_wrt_{COLUMN}_{window}\")\n",
    "    count_df[COLUMNS].to_parquet(f\"{data_dir}{RESULT_FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = pd.read_parquet(f\"{data_dir}{BASE_FILE_NAME}_test.parquet\")\n",
    "count_for_prop_df = pd.read_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")\n",
    "COLUMNS = [\"id\"]\n",
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        count_df[f\"prop_{target}_wrt_{COLUMN}_{window}\"] = \\\n",
    "            count_for_prop_df[f'{target}_count_for_prop_wrt_{COLUMN}_{window}'] / \\\n",
    "            count_df[f'{BASE_FILE_NAME}_{window}']\n",
    "        COLUMNS.append(f\"prop_{target}_wrt_{COLUMN}_{window}\")\n",
    "count_df[COLUMNS].to_parquet(f\"{data_dir}{RESULT_FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ratio: unique('PAYR_IP')/unique('MPHN_NO'), unique('CP_CD')/unique('PAYR_SEQ'), unique('PAYR_IP')/unique('PAYR_SEQ'), unique('MPHN_NO')/unique('PAYR_SEQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 05:27:44.041148\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_FILE_NAME = \"unique_count_wrt_gods\"\n",
    "FILE_NAME = \"unique_divide_wrt_gods\"\n",
    "COLUMN = \"GODS_NM\"\n",
    "COLUMNS = []\n",
    "TARGET_LIST = [\"CP_CD\", \"PAYR_SEQ\", \"PAYR_IP\", \"MPHN_NO\"]\n",
    "NEW_TARGET_LIST = [(\"PAYR_IP\", \"MPHN_NO\"), (\"CP_CD\", \"PAYR_SEQ\"), (\"PAYR_IP\", \"PAYR_SEQ\"), (\"MPHN_NO\", \"PAYR_SEQ\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        COLUMNS.append(f'unique_{target}_wrt_{COLUMN}_{window}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    df_train = pd.read_parquet(f\"{data_dir}{LOAD_FILE_NAME}_{seed}.parquet\")[[\"id\"] + COLUMNS]\n",
    "    new_columns = []\n",
    "    for window in WINDOW_SIZE:\n",
    "        for new_target in NEW_TARGET_LIST:\n",
    "            new_column = f\"unique_{new_target[0]}_by_{new_target[1]}_wrt_{COLUMN}_{window}\"\n",
    "            df_train[new_column] = df_train[f\"unique_{new_target[0]}_wrt_{COLUMN}_{window}\"] / df_train[f\"unique_{new_target[1]}_wrt_{COLUMN}_{window}\"]\n",
    "            new_columns.append(new_column)\n",
    "    df_train = df_train[[\"id\"] + new_columns].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet(f\"{data_dir}{LOAD_FILE_NAME}_test.parquet\")[[\"id\"] + COLUMNS]\n",
    "\n",
    "new_columns = []\n",
    "for window in WINDOW_SIZE:\n",
    "    for new_target in NEW_TARGET_LIST:\n",
    "        new_column = f\"unique_{new_target[0]}_by_{new_target[1]}_wrt_{COLUMN}_{window}\"\n",
    "        df_test[new_column] = df_test[f\"unique_{new_target[0]}_wrt_{COLUMN}_{window}\"] / df_test[f\"unique_{new_target[1]}_wrt_{COLUMN}_{window}\"]\n",
    "        new_columns.append(new_column)\n",
    "df_test = df_test[[\"id\"] + new_columns].to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-12 12:17:26.079300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User(PAYR_SEQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 05:28:38.347491\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"seq_count\"\n",
    "COLUMN = \"PAYR_SEQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD']], test_df[['id', COLUMN, 'REQ_DD']]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})\n",
    "df_test = test_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window size: 3D\n",
      "window size: 10D\n",
      "window size: 30D\n"
     ]
    }
   ],
   "source": [
    "for window_size in WINDOW_SIZE:\n",
    "    print(f\"window size: {window_size}D\")\n",
    "    count = df.groupby([COLUMN, 'REQ_DD']).count().reset_index().rename(columns={\"id\": \"count\"})\n",
    "    count['datetime'] = pd.to_datetime(count['REQ_DD'], format='%Y%m%d')\n",
    "    count = count.drop(columns=['REQ_DD'])\n",
    "    cum_count = count.sort_values(by=['datetime']).groupby([COLUMN]).rolling(f\"{window_size+1}D\", min_periods=1, on='datetime')['count'].sum().reset_index().rename(columns={\"count\": f\"cum_{window_size}_count\"})\n",
    "    cum_count['REQ_DD'] = cum_count['datetime'].dt.strftime('%Y%m%d')\n",
    "    cum_count = cum_count.drop(columns=['datetime'])\n",
    "    count['REQ_DD'] = count['datetime'].dt.strftime('%Y%m%d')\n",
    "    count = count.drop(columns=['datetime'])\n",
    "    count = count.merge(cum_count, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "    count[f'{FILE_NAME}_{window_size}'] = count[f'cum_{window_size}_count'] - count['count']\n",
    "    count = count[[COLUMN, 'REQ_DD', f'{FILE_NAME}_{window_size}']].astype({'REQ_DD': 'int32'})\n",
    "    df_train = df_train.merge(count, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "    df_test = df_test.merge(count, on=[COLUMN, 'REQ_DD'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=['REQ_DD', COLUMN])\n",
    "df_test = df_test.drop(columns=['REQ_DD', COLUMN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-12 08:25:26.436829\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unique count of 'CP_CD', 'GODS_NM', 'PAYR_IP', 'MPHN_NO', 'COMMC_CLF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 06:00:07.688342\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"unique_count_wrt_seq\"\n",
    "COLUMN = \"PAYR_SEQ\"\n",
    "TARGET_LIST = [\"CP_CD\", \"GODS_NM\", \"PAYR_IP\", \"MPHN_NO\", \"COMMC_CLF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST], test_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST]], \n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: CP_CD\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_CP_CD_count_wrt_PAYR_SEQ_3 exists!\n",
      "window: 3, target: GODS_NM\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_GODS_NM_count_wrt_PAYR_SEQ_3 exists!\n",
      "window: 3, target: PAYR_IP\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_PAYR_IP_count_wrt_PAYR_SEQ_3 exists!\n",
      "window: 3, target: MPHN_NO\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_MPHN_NO_count_wrt_PAYR_SEQ_3 exists!\n",
      "window: 3, target: COMMC_CLF\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_COMMC_CLF_count_wrt_PAYR_SEQ_3 exists!\n",
      "window: 10, target: CP_CD\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_CP_CD_count_wrt_PAYR_SEQ_10 exists!\n",
      "window: 10, target: GODS_NM\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_GODS_NM_count_wrt_PAYR_SEQ_10 exists!\n",
      "window: 10, target: PAYR_IP\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_PAYR_IP_count_wrt_PAYR_SEQ_10 exists!\n",
      "window: 10, target: MPHN_NO\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_MPHN_NO_count_wrt_PAYR_SEQ_10 exists!\n",
      "window: 10, target: COMMC_CLF\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_COMMC_CLF_count_wrt_PAYR_SEQ_10 exists!\n",
      "window: 30, target: CP_CD\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_CP_CD_count_wrt_PAYR_SEQ_30 exists!\n",
      "window: 30, target: GODS_NM\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_GODS_NM_count_wrt_PAYR_SEQ_30 exists!\n",
      "window: 30, target: PAYR_IP\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_PAYR_IP_count_wrt_PAYR_SEQ_30 exists!\n",
      "window: 30, target: MPHN_NO\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_MPHN_NO_count_wrt_PAYR_SEQ_30 exists!\n",
      "window: 30, target: COMMC_CLF\n",
      "/home/workspace/user-workspace/junheon/data/task150/unique_COMMC_CLF_count_wrt_PAYR_SEQ_30 exists!\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}unique_{target}_count_wrt_{COLUMN}_{window}\"\n",
    "        if path.exists(data_path):\n",
    "            print(f\"{data_path} exists!\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        temp_df = df[[COLUMN, target, 'REQ_DD']].drop_duplicates(subset=[COLUMN, target, 'REQ_DD']).sort_values(by=['REQ_DD'])\n",
    "        temp_df['datetime'] = pd.to_datetime(temp_df['REQ_DD'], format='%Y%m%d')\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "        \n",
    "        count_list = []\n",
    "        start_date = \"2019-07-01\"\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        while True:\n",
    "            end_date = start_date + datetime.timedelta(days=window)\n",
    "\n",
    "            if end_date > datetime.datetime.strptime(\"2019-12-31\", \"%Y-%m-%d\"):\n",
    "                break\n",
    "\n",
    "            count_df = temp_df[(temp_df['datetime']>=start_date)&(temp_df['datetime']<end_date)][[COLUMN, target]]\n",
    "            count_df = count_df.groupby([COLUMN]).agg(['nunique']).reset_index()\n",
    "            count_df.columns = [COLUMN, f'unique_{target}_wrt_{COLUMN}_{window}']\n",
    "            count_df['REQ_DD'] = datetime.datetime.strftime(end_date, \"%Y%m%d\")\n",
    "\n",
    "            count_list.append(count_df)\n",
    "            start_date = start_date + datetime.timedelta(days=1)\n",
    "        count_df = pd.concat(count_list, axis=0)\n",
    "        count_df.to_parquet(data_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: CP_CD\n",
      "window: 3, target: GODS_NM\n",
      "window: 3, target: PAYR_IP\n",
      "window: 3, target: MPHN_NO\n",
      "window: 3, target: COMMC_CLF\n",
      "window: 10, target: CP_CD\n",
      "window: 10, target: GODS_NM\n",
      "window: 10, target: PAYR_IP\n",
      "window: 10, target: MPHN_NO\n",
      "window: 10, target: COMMC_CLF\n",
      "window: 30, target: CP_CD\n",
      "window: 30, target: GODS_NM\n",
      "window: 30, target: PAYR_IP\n",
      "window: 30, target: MPHN_NO\n",
      "window: 30, target: COMMC_CLF\n",
      "window: 3, target: CP_CD\n",
      "window: 3, target: GODS_NM\n",
      "window: 3, target: PAYR_IP\n",
      "window: 3, target: MPHN_NO\n",
      "window: 3, target: COMMC_CLF\n",
      "window: 10, target: CP_CD\n",
      "window: 10, target: GODS_NM\n",
      "window: 10, target: PAYR_IP\n",
      "window: 10, target: MPHN_NO\n",
      "window: 10, target: COMMC_CLF\n",
      "window: 30, target: CP_CD\n",
      "window: 30, target: GODS_NM\n",
      "window: 30, target: PAYR_IP\n",
      "window: 30, target: MPHN_NO\n",
      "window: 30, target: COMMC_CLF\n",
      "window: 3, target: CP_CD\n",
      "window: 3, target: GODS_NM\n",
      "window: 3, target: PAYR_IP\n",
      "window: 3, target: MPHN_NO\n",
      "window: 3, target: COMMC_CLF\n",
      "window: 10, target: CP_CD\n",
      "window: 10, target: GODS_NM\n",
      "window: 10, target: PAYR_IP\n",
      "window: 10, target: MPHN_NO\n",
      "window: 10, target: COMMC_CLF\n",
      "window: 30, target: CP_CD\n",
      "window: 30, target: GODS_NM\n",
      "window: 30, target: PAYR_IP\n",
      "window: 30, target: MPHN_NO\n",
      "window: 30, target: COMMC_CLF\n",
      "window: 3, target: CP_CD\n",
      "window: 3, target: GODS_NM\n",
      "window: 3, target: PAYR_IP\n",
      "window: 3, target: MPHN_NO\n",
      "window: 3, target: COMMC_CLF\n",
      "window: 10, target: CP_CD\n",
      "window: 10, target: GODS_NM\n",
      "window: 10, target: PAYR_IP\n",
      "window: 10, target: MPHN_NO\n",
      "window: 10, target: COMMC_CLF\n",
      "window: 30, target: CP_CD\n",
      "window: 30, target: GODS_NM\n",
      "window: 30, target: PAYR_IP\n",
      "window: 30, target: MPHN_NO\n",
      "window: 30, target: COMMC_CLF\n",
      "window: 3, target: CP_CD\n",
      "window: 3, target: GODS_NM\n",
      "window: 3, target: PAYR_IP\n",
      "window: 3, target: MPHN_NO\n",
      "window: 3, target: COMMC_CLF\n",
      "window: 10, target: CP_CD\n",
      "window: 10, target: GODS_NM\n",
      "window: 10, target: PAYR_IP\n",
      "window: 10, target: MPHN_NO\n",
      "window: 10, target: COMMC_CLF\n",
      "window: 30, target: CP_CD\n",
      "window: 30, target: GODS_NM\n",
      "window: 30, target: PAYR_IP\n",
      "window: 30, target: MPHN_NO\n",
      "window: 30, target: COMMC_CLF\n"
     ]
    }
   ],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train = train_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'}).iloc[indices]\n",
    "    \n",
    "    for window in WINDOW_SIZE:\n",
    "        for target in TARGET_LIST:\n",
    "            print(f\"window: {window}, target: {target}\")\n",
    "            data_path = f\"{data_dir}unique_{target}_count_wrt_{COLUMN}_{window}\"\n",
    "\n",
    "            count_df = pd.read_parquet(data_path).astype({'REQ_DD': 'int32'})\n",
    "            df_train = df_train.merge(count_df, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "    \n",
    "    df_train.drop(columns=[\"REQ_DD\", COLUMN]).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = test_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})\n",
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}unique_{target}_count_wrt_{COLUMN}_{window}\"\n",
    "        \n",
    "        count_df = pd.read_parquet(data_path).astype({'REQ_DD': 'int32'})\n",
    "        df_test = df_test.merge(count_df, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "df_test.drop(columns=[\"REQ_DD\", COLUMN]).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 마지막 거래일까지의 diff (window X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 07:21:49.739012\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"last_transaction_diff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REQ_DD', \"PAYR_SEQ\", \"MPHN_NO\"]].astype({'REQ_DD': 'int32'})\n",
    "df_test = test_df[['id', 'REQ_DD', \"PAYR_SEQ\", \"MPHN_NO\"]].astype({'REQ_DD': 'int32'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAYR_SEQ 기준 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN = \"PAYR_SEQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD']], test_df[['id', COLUMN, 'REQ_DD']]], \n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_df = df.drop_duplicates(subset=[COLUMN, 'REQ_DD']).sort_values(by=[COLUMN, \"REQ_DD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_df['datetime'] = pd.to_datetime(prev_df['REQ_DD'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_df[f'prev_{COLUMN}'] = prev_df[COLUMN].shift(1)\n",
    "prev_df[f'prev_datetime'] = prev_df['datetime'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_df[f'diff_wrt_{COLUMN}'] = (prev_df['datetime']-prev_df['prev_datetime']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_df[f'diff_wrt_{COLUMN}'] = np.where((prev_df[COLUMN]==prev_df[f\"prev_{COLUMN}\"]), prev_df[f'diff_wrt_{COLUMN}'], 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_df = prev_df[[COLUMN, \"REQ_DD\", f\"diff_wrt_{COLUMN}\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.merge(prev_df, on=[f\"{COLUMN}\", \"REQ_DD\"], how='left').drop(columns=[COLUMN])\n",
    "df_test = df_test.merge(prev_df, on=[f\"{COLUMN}\", \"REQ_DD\"], how='left').drop(columns=[COLUMN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPHN_NO 기준 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN = \"MPHN_NO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD']], test_df[['id', COLUMN, 'REQ_DD']]], \n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_df = df.drop_duplicates(subset=[COLUMN, 'REQ_DD']).sort_values(by=[COLUMN, \"REQ_DD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_df['datetime'] = pd.to_datetime(prev_df['REQ_DD'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_df[f'prev_{COLUMN}'] = prev_df[COLUMN].shift(1)\n",
    "prev_df[f'prev_datetime'] = prev_df['datetime'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_df[f'diff_wrt_{COLUMN}'] = (prev_df['datetime']-prev_df['prev_datetime']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_df[f'diff_wrt_{COLUMN}'] = np.where((prev_df[COLUMN]==prev_df[f\"prev_{COLUMN}\"]), prev_df[f'diff_wrt_{COLUMN}'], 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_df = prev_df[[COLUMN, \"REQ_DD\", f\"diff_wrt_{COLUMN}\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.merge(prev_df, on=[f\"{COLUMN}\", \"REQ_DD\"], how='left').drop(columns=[COLUMN])\n",
    "df_test = df_test.merge(prev_df, on=[f\"{COLUMN}\", \"REQ_DD\"], how='left').drop(columns=[COLUMN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=[\"REQ_DD\"])\n",
    "df_test = df_test.drop(columns=[\"REQ_DD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IP AND SEQ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ip_count_wrt_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 07:23:02.419519\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"ip_count_wrt_seq\"\n",
    "COLUMN = \"PAYR_SEQ\"\n",
    "TARGET = \"PAYR_IP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD', TARGET]], test_df[['id', COLUMN, 'REQ_DD', TARGET]]], \n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: PAYR_IP\n",
      "/home/workspace/user-workspace/junheon/data/task150/ip_count_wrt_seq_window3 exists!\n",
      "window: 10, target: PAYR_IP\n",
      "/home/workspace/user-workspace/junheon/data/task150/ip_count_wrt_seq_window10 exists!\n",
      "window: 30, target: PAYR_IP\n",
      "/home/workspace/user-workspace/junheon/data/task150/ip_count_wrt_seq_window30 exists!\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    print(f\"window: {window}, target: {TARGET}\")\n",
    "    data_path = f\"{data_dir}{FILE_NAME}_window{window}\"\n",
    "    if path.exists(data_path):\n",
    "        print(f\"{data_path} exists!\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    temp_df = df[[COLUMN, TARGET, 'REQ_DD']].sort_values(by=['REQ_DD'])\n",
    "    temp_df['datetime'] = pd.to_datetime(temp_df['REQ_DD'], format='%Y%m%d')\n",
    "    temp_df = temp_df.reset_index(drop=True)\n",
    "\n",
    "    count_list = []\n",
    "    start_date = \"2019-07-01\"\n",
    "    start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    while True:\n",
    "        end_date = start_date + datetime.timedelta(days=window)\n",
    "\n",
    "        if end_date > datetime.datetime.strptime(\"2019-12-31\", \"%Y-%m-%d\"):\n",
    "            break\n",
    "\n",
    "        count_df = temp_df[(temp_df['datetime']>=start_date)&(temp_df['datetime']<end_date)][[COLUMN, TARGET]]\n",
    "        count_df = count_df.groupby([COLUMN]).count().reset_index()\n",
    "        count_df.columns = [COLUMN, f'count_{TARGET}_wrt_{COLUMN}_{window}']\n",
    "        count_df['REQ_DD'] = datetime.datetime.strftime(end_date, \"%Y%m%d\")\n",
    "\n",
    "        count_list.append(count_df)\n",
    "        start_date = start_date + datetime.timedelta(days=1)\n",
    "    count_df = pd.concat(count_list, axis=0)\n",
    "    count_df.to_parquet(data_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: PAYR_IP\n",
      "window: 10, target: PAYR_IP\n",
      "window: 30, target: PAYR_IP\n",
      "window: 3, target: PAYR_IP\n",
      "window: 10, target: PAYR_IP\n",
      "window: 30, target: PAYR_IP\n",
      "window: 3, target: PAYR_IP\n",
      "window: 10, target: PAYR_IP\n",
      "window: 30, target: PAYR_IP\n",
      "window: 3, target: PAYR_IP\n",
      "window: 10, target: PAYR_IP\n",
      "window: 30, target: PAYR_IP\n",
      "window: 3, target: PAYR_IP\n",
      "window: 10, target: PAYR_IP\n",
      "window: 30, target: PAYR_IP\n"
     ]
    }
   ],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train = train_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'}).iloc[indices]\n",
    "    \n",
    "    for window in WINDOW_SIZE:\n",
    "        print(f\"window: {window}, target: {TARGET}\")\n",
    "        data_path = f\"{data_dir}{FILE_NAME}_window{window}\"\n",
    "\n",
    "        count_df = pd.read_parquet(data_path).astype({'REQ_DD': 'int32'})\n",
    "        df_train = df_train.merge(count_df, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "    \n",
    "    df_train.drop(columns=[\"REQ_DD\", COLUMN]).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = test_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})\n",
    "for window in WINDOW_SIZE:\n",
    "    print(f\"window: {window}, target: {TARGET}\")\n",
    "    data_path = data_path = f\"{data_dir}{FILE_NAME}_window{window}\"\n",
    "\n",
    "    count_df = pd.read_parquet(data_path).astype({'REQ_DD': 'int32'})\n",
    "    df_test = df_test.merge(count_df, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "df_test.drop(columns=[\"REQ_DD\", COLUMN]).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq_count_wrt_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 07:44:34.042583\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"seq_count_wrt_ip\"\n",
    "COLUMN = \"PAYR_IP\"\n",
    "TARGET = \"PAYR_SEQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD', TARGET]], test_df[['id', COLUMN, 'REQ_DD', TARGET]]], \n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: PAYR_SEQ\n",
      "/home/workspace/user-workspace/junheon/data/task150/seq_count_wrt_ip_window3 exists!\n",
      "window: 10, target: PAYR_SEQ\n",
      "/home/workspace/user-workspace/junheon/data/task150/seq_count_wrt_ip_window10 exists!\n",
      "window: 30, target: PAYR_SEQ\n",
      "/home/workspace/user-workspace/junheon/data/task150/seq_count_wrt_ip_window30 exists!\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    print(f\"window: {window}, target: {TARGET}\")\n",
    "    data_path = f\"{data_dir}{FILE_NAME}_window{window}\"\n",
    "    if path.exists(data_path):\n",
    "        print(f\"{data_path} exists!\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    temp_df = df[[COLUMN, TARGET, 'REQ_DD']].sort_values(by=['REQ_DD'])\n",
    "    temp_df['datetime'] = pd.to_datetime(temp_df['REQ_DD'], format='%Y%m%d')\n",
    "    temp_df = temp_df.reset_index(drop=True)\n",
    "\n",
    "    count_list = []\n",
    "    start_date = \"2019-07-01\"\n",
    "    start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    while True:\n",
    "        end_date = start_date + datetime.timedelta(days=window)\n",
    "\n",
    "        if end_date > datetime.datetime.strptime(\"2019-12-31\", \"%Y-%m-%d\"):\n",
    "            break\n",
    "\n",
    "        count_df = temp_df[(temp_df['datetime']>=start_date)&(temp_df['datetime']<end_date)][[COLUMN, TARGET]]\n",
    "        count_df = count_df.groupby([COLUMN]).count().reset_index()\n",
    "        count_df.columns = [COLUMN, f'count_{TARGET}_wrt_{COLUMN}_{window}']\n",
    "        count_df['REQ_DD'] = datetime.datetime.strftime(end_date, \"%Y%m%d\")\n",
    "\n",
    "        count_list.append(count_df)\n",
    "        start_date = start_date + datetime.timedelta(days=1)\n",
    "    count_df = pd.concat(count_list, axis=0)\n",
    "    count_df.to_parquet(data_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: PAYR_SEQ\n",
      "window: 10, target: PAYR_SEQ\n",
      "window: 30, target: PAYR_SEQ\n",
      "window: 3, target: PAYR_SEQ\n",
      "window: 10, target: PAYR_SEQ\n",
      "window: 30, target: PAYR_SEQ\n",
      "window: 3, target: PAYR_SEQ\n",
      "window: 10, target: PAYR_SEQ\n",
      "window: 30, target: PAYR_SEQ\n",
      "window: 3, target: PAYR_SEQ\n",
      "window: 10, target: PAYR_SEQ\n",
      "window: 30, target: PAYR_SEQ\n",
      "window: 3, target: PAYR_SEQ\n",
      "window: 10, target: PAYR_SEQ\n",
      "window: 30, target: PAYR_SEQ\n"
     ]
    }
   ],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train = train_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'}).iloc[indices]\n",
    "    \n",
    "    for window in WINDOW_SIZE:\n",
    "        print(f\"window: {window}, target: {TARGET}\")\n",
    "        data_path = f\"{data_dir}{FILE_NAME}_window{window}\"\n",
    "\n",
    "        count_df = pd.read_parquet(data_path).astype({'REQ_DD': 'int32'})\n",
    "        df_train = df_train.merge(count_df, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "    \n",
    "    df_train.drop(columns=[\"REQ_DD\", COLUMN]).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = test_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})\n",
    "for window in WINDOW_SIZE:\n",
    "    print(f\"window: {window}, target: {TARGET}\")\n",
    "    data_path = data_path = f\"{data_dir}{FILE_NAME}_window{window}\"\n",
    "\n",
    "    count_df = pd.read_parquet(data_path).astype({'REQ_DD': 'int32'})\n",
    "    df_test = df_test.merge(count_df, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "df_test.drop(columns=[\"REQ_DD\", COLUMN]).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 07:57:37.970057\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"newness\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAYR_SEQ, CP_CD, PAYR_IP, MPHN_NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LIST = [\"PAYR_SEQ\", \"CP_CD\", \"PAYR_IP\", \"MPHN_NO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in TARGET_LIST:\n",
    "    prev_df = pd.concat([train_df[[target, 'REQ_DD']], test_df[[target, 'REQ_DD']]], ignore_index=True)\n",
    "    prev_df = prev_df.drop_duplicates(subset=[target, 'REQ_DD']).sort_values(by=[target, \"REQ_DD\"])\n",
    "    prev_df[\"cum_count\"] = prev_df.groupby(target).cumcount()\n",
    "    prev_df[f'newness_{target}'] = np.where((prev_df[\"cum_count\"]>0), 0, 1)\n",
    "    prev_df[[target, \"REQ_DD\", f\"newness_{target}\"]].to_parquet(f\"{data_dir}newness_{target}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REQ_DD'] + TARGET_LIST].astype({'REQ_DD': 'int32'})\n",
    "df_test = test_df[['id', 'REQ_DD'] + TARGET_LIST].astype({'REQ_DD': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in TARGET_LIST:\n",
    "    prev_df = pd.read_parquet(f\"{data_dir}newness_{target}.parquet\")\n",
    "    df_train = df_train.merge(prev_df, on=[\"REQ_DD\", target], how='left')\n",
    "    df_test = df_test.merge(prev_df, on=[\"REQ_DD\", target], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=TARGET_LIST + ['REQ_DD'])\n",
    "df_test = df_test.drop(columns=TARGET_LIST + ['REQ_DD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA로부터 얻은 것들 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CP_M_CLF_NM가 후원/기부 인지 여부 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 07:59:21.341120\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"filter_CP_M_CLF_NM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "donate_num = decoder['CP_M_CLF_NM']['후원/기부']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donate_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'CP_M_CLF_NM']]\n",
    "df_test = test_df[['id', 'CP_M_CLF_NM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-63-8807684a90bf>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_donate\"] = (df_train['CP_M_CLF_NM']==donate_num)\n",
      "<ipython-input-63-8807684a90bf>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_donate\"] = (df_test['CP_M_CLF_NM']==donate_num)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"is_donate\"] = (df_train['CP_M_CLF_NM']==donate_num)\n",
    "df_test[\"is_donate\"] = (df_test['CP_M_CLF_NM']==donate_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'is_donate']]\n",
    "df_test = df_test[['id', 'is_donate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMD_LMT_AMT가 60만원 이상인지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 07:59:22.507385\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"filter_REMD_LMT_AMT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REMD_LMT_AMT']]\n",
    "df_test = test_df[['id', 'REMD_LMT_AMT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-69-f2a0c9a28a4d>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"big_REMD_LMT_AMT\"] = (df_train['REMD_LMT_AMT'] > 600000)\n",
      "<ipython-input-69-f2a0c9a28a4d>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"big_REMD_LMT_AMT\"] = (df_test['REMD_LMT_AMT'] > 600000)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"big_REMD_LMT_AMT\"] = (df_train['REMD_LMT_AMT'] > 600000)\n",
    "df_test[\"big_REMD_LMT_AMT\"] = (df_test['REMD_LMT_AMT'] > 600000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'big_REMD_LMT_AMT']]\n",
    "df_test = df_test[['id', 'big_REMD_LMT_AMT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACUM_RCPT_AMT가 1 미만인지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 07:59:23.576649\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"filter_ACUM_RCPT_AMT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'ACUM_RCPT_AMT']]\n",
    "df_test = test_df[['id', 'ACUM_RCPT_AMT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-75-11e88f95ae9a>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"negative_ACUM_RCPT_AMT\"] = (df_train['ACUM_RCPT_AMT'] < 1)\n",
      "<ipython-input-75-11e88f95ae9a>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"negative_ACUM_RCPT_AMT\"] = (df_test['ACUM_RCPT_AMT'] < 1)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"negative_ACUM_RCPT_AMT\"] = (df_train['ACUM_RCPT_AMT'] < 1)\n",
    "df_test[\"negative_ACUM_RCPT_AMT\"] = (df_test['ACUM_RCPT_AMT'] < 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'negative_ACUM_RCPT_AMT']]\n",
    "df_test = df_test[['id', 'negative_ACUM_RCPT_AMT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CP_S_CLF_NM이 high risk list에 포함되는지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 07:59:24.610860\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"filter_CP_S_CLF_NM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'CP_S_CLF_NM']]\n",
    "df_test = test_df[['id', 'CP_S_CLF_NM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_risk_list = train_df[train_df['target']==1]['CP_S_CLF_NM'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-82-543884bf3480>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"high_risk_CP_S_CLF_NM\"] = df_train['CP_S_CLF_NM'].isin(high_risk_list)\n",
      "<ipython-input-82-543884bf3480>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"high_risk_CP_S_CLF_NM\"] = df_test['CP_S_CLF_NM'].isin(high_risk_list)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"high_risk_CP_S_CLF_NM\"] = df_train['CP_S_CLF_NM'].isin(high_risk_list)\n",
    "df_test[\"high_risk_CP_S_CLF_NM\"] = df_test['CP_S_CLF_NM'].isin(high_risk_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'high_risk_CP_S_CLF_NM']]\n",
    "df_test = df_test[['id', 'high_risk_CP_S_CLF_NM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IP_SUB_A가 high risk list에 포함되는지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 07:59:26.220557\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"filter_SUB_IP_A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'SUB_IP_A']]\n",
    "df_test = test_df[['id', 'SUB_IP_A']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_risk_list = train_df[train_df['target']==1]['SUB_IP_A'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-89-d3e396c3a12c>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"high_risk_SUB_IP_A\"] = df_train['SUB_IP_A'].isin(high_risk_list)\n",
      "<ipython-input-89-d3e396c3a12c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"high_risk_SUB_IP_A\"] = df_test['SUB_IP_A'].isin(high_risk_list)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"high_risk_SUB_IP_A\"] = df_train['SUB_IP_A'].isin(high_risk_list)\n",
    "df_test[\"high_risk_SUB_IP_A\"] = df_test['SUB_IP_A'].isin(high_risk_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'high_risk_SUB_IP_A']]\n",
    "df_test = df_test[['id', 'high_risk_SUB_IP_A']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AC_PAY_AMT == 49900, 50000, 99000, 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"bin_AC_PAY_AMT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'AC_PAY_AMT']]\n",
    "df_test = test_df[['id', 'AC_PAY_AMT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-100-e72358736fd3>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_AC_PAY_AMT_49900\"] = (df_train['AC_PAY_AMT'] == 49900)\n",
      "<ipython-input-100-e72358736fd3>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_AC_PAY_AMT_49900\"] = (df_test['AC_PAY_AMT'] == 49900)\n",
      "<ipython-input-100-e72358736fd3>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_AC_PAY_AMT_50000\"] = (df_train['AC_PAY_AMT'] == 50000)\n",
      "<ipython-input-100-e72358736fd3>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_AC_PAY_AMT_50000\"] = (df_test['AC_PAY_AMT'] == 50000)\n",
      "<ipython-input-100-e72358736fd3>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_AC_PAY_AMT_99000\"] = (df_train['AC_PAY_AMT'] == 99000)\n",
      "<ipython-input-100-e72358736fd3>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_AC_PAY_AMT_99000\"] = (df_test['AC_PAY_AMT'] == 99000)\n",
      "<ipython-input-100-e72358736fd3>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_AC_PAY_AMT_100000\"] = (df_train['AC_PAY_AMT'] == 100000)\n",
      "<ipython-input-100-e72358736fd3>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_AC_PAY_AMT_100000\"] = (df_test['AC_PAY_AMT'] == 100000)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"is_AC_PAY_AMT_49900\"] = (df_train['AC_PAY_AMT'] == 49900)\n",
    "df_test[\"is_AC_PAY_AMT_49900\"] = (df_test['AC_PAY_AMT'] == 49900)\n",
    "df_train[\"is_AC_PAY_AMT_50000\"] = (df_train['AC_PAY_AMT'] == 50000)\n",
    "df_test[\"is_AC_PAY_AMT_50000\"] = (df_test['AC_PAY_AMT'] == 50000)\n",
    "df_train[\"is_AC_PAY_AMT_99000\"] = (df_train['AC_PAY_AMT'] == 99000)\n",
    "df_test[\"is_AC_PAY_AMT_99000\"] = (df_test['AC_PAY_AMT'] == 99000)\n",
    "df_train[\"is_AC_PAY_AMT_100000\"] = (df_train['AC_PAY_AMT'] == 100000)\n",
    "df_test[\"is_AC_PAY_AMT_100000\"] = (df_test['AC_PAY_AMT'] == 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'is_AC_PAY_AMT_49900', 'is_AC_PAY_AMT_50000', 'is_AC_PAY_AMT_99000', 'is_AC_PAY_AMT_100000']]\n",
    "df_test = df_test[['id', 'is_AC_PAY_AMT_49900', 'is_AC_PAY_AMT_50000', 'is_AC_PAY_AMT_99000', 'is_AC_PAY_AMT_100000']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MM_LMT_AMT  == 600000, 40000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"bin_MM_LMT_AMT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'MM_LMT_AMT']]\n",
    "df_test = test_df[['id', 'MM_LMT_AMT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-106-9d0d5f1e4dae>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_MM_LMT_AMT_600000\"] = (df_train['MM_LMT_AMT'] == 600000)\n",
      "<ipython-input-106-9d0d5f1e4dae>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_MM_LMT_AMT_600000\"] = (df_test['MM_LMT_AMT'] == 600000)\n",
      "<ipython-input-106-9d0d5f1e4dae>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_MM_LMT_AMT_40000\"] = (df_train['MM_LMT_AMT'] == 40000)\n",
      "<ipython-input-106-9d0d5f1e4dae>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_MM_LMT_AMT_40000\"] = (df_test['MM_LMT_AMT'] == 40000)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"is_MM_LMT_AMT_600000\"] = (df_train['MM_LMT_AMT'] == 600000)\n",
    "df_test[\"is_MM_LMT_AMT_600000\"] = (df_test['MM_LMT_AMT'] == 600000)\n",
    "df_train[\"is_MM_LMT_AMT_40000\"] = (df_train['MM_LMT_AMT'] == 40000)\n",
    "df_test[\"is_MM_LMT_AMT_40000\"] = (df_test['MM_LMT_AMT'] == 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'is_MM_LMT_AMT_600000', 'is_MM_LMT_AMT_40000']]\n",
    "df_test = df_test[['id', 'is_MM_LMT_AMT_600000', 'is_MM_LMT_AMT_40000']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMD_LMT_AMT == 30만원/60만원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"bin_REMD_LMT_AMT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REMD_LMT_AMT']]\n",
    "df_test = test_df[['id', 'REMD_LMT_AMT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-111-c4a1ef26e860>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_REMD_LMT_AMT_600000\"] = (df_train['REMD_LMT_AMT'] == 600000)\n",
      "<ipython-input-111-c4a1ef26e860>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_REMD_LMT_AMT_600000\"] = (df_test['REMD_LMT_AMT'] == 600000)\n",
      "<ipython-input-111-c4a1ef26e860>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_REMD_LMT_AMT_300000\"] = (df_train['REMD_LMT_AMT'] == 40000)\n",
      "<ipython-input-111-c4a1ef26e860>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_REMD_LMT_AMT_300000\"] = (df_test['REMD_LMT_AMT'] == 40000)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"is_REMD_LMT_AMT_600000\"] = (df_train['REMD_LMT_AMT'] == 600000)\n",
    "df_test[\"is_REMD_LMT_AMT_600000\"] = (df_test['REMD_LMT_AMT'] == 600000)\n",
    "df_train[\"is_REMD_LMT_AMT_300000\"] = (df_train['REMD_LMT_AMT'] == 40000)\n",
    "df_test[\"is_REMD_LMT_AMT_300000\"] = (df_test['REMD_LMT_AMT'] == 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'is_REMD_LMT_AMT_600000', 'is_REMD_LMT_AMT_300000']]\n",
    "df_test = df_test[['id', 'is_REMD_LMT_AMT_600000', 'is_REMD_LMT_AMT_300000']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGE < 26 , 27<=AGE<=31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"bin_AGE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'AGE']]\n",
    "df_test = test_df[['id', 'AGE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-116-ba106006ecfb>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"AGE_under_26\"] = (df_train['AGE'] < 26)\n",
      "<ipython-input-116-ba106006ecfb>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"AGE_under_26\"] = (df_test['AGE'] < 26)\n",
      "<ipython-input-116-ba106006ecfb>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"AGE_27_31\"] = ((df_train['AGE'] > 26) & (df_train['AGE'] < 32))\n",
      "<ipython-input-116-ba106006ecfb>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"AGE_27_31\"] = ((df_test['AGE'] > 26) & (df_test['AGE'] < 32))\n"
     ]
    }
   ],
   "source": [
    "df_train[\"AGE_under_26\"] = (df_train['AGE'] < 26)\n",
    "df_test[\"AGE_under_26\"] = (df_test['AGE'] < 26)\n",
    "df_train[\"AGE_27_31\"] = ((df_train['AGE'] > 26) & (df_train['AGE'] < 32))\n",
    "df_test[\"AGE_27_31\"] = ((df_test['AGE'] > 26) & (df_test['AGE'] < 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'AGE_under_26', 'AGE_27_31']]\n",
    "df_test = df_test[['id', 'AGE_under_26', 'AGE_27_31']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTHTI_CLF_FLG == A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"bin_AUTHTI_CLF_FLG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'AUTHTI_CLF_FLG']]\n",
    "df_test = test_df[['id', 'AUTHTI_CLF_FLG']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-121-33529c421a1a>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_AUTHTI_CLF_FLG_A\"] = (df_train['AUTHTI_CLF_FLG'] == \"A\")\n",
      "<ipython-input-121-33529c421a1a>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_AUTHTI_CLF_FLG_A\"] = (df_test['AUTHTI_CLF_FLG'] == \"A\")\n"
     ]
    }
   ],
   "source": [
    "df_train[\"is_AUTHTI_CLF_FLG_A\"] = (df_train['AUTHTI_CLF_FLG'] == \"A\")\n",
    "df_test[\"is_AUTHTI_CLF_FLG_A\"] = (df_test['AUTHTI_CLF_FLG'] == \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'is_AUTHTI_CLF_FLG_A']]\n",
    "df_test = df_test[['id', 'is_AUTHTI_CLF_FLG_A']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACUM_RCPT_AMT == 0, 11000, 49900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"bin_ACUM_RCPT_AMT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'ACUM_RCPT_AMT']]\n",
    "df_test = test_df[['id', 'ACUM_RCPT_AMT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-126-e7011a4bfa49>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_ACUM_RCPT_AMT_0\"] = (df_train['ACUM_RCPT_AMT'] == 0)\n",
      "<ipython-input-126-e7011a4bfa49>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_ACUM_RCPT_AMT_0\"] = (df_test['ACUM_RCPT_AMT'] == 0)\n",
      "<ipython-input-126-e7011a4bfa49>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_ACUM_RCPT_AMT_11000\"] = (df_train['ACUM_RCPT_AMT'] == 11000)\n",
      "<ipython-input-126-e7011a4bfa49>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_ACUM_RCPT_AMT_11000\"] = (df_test['ACUM_RCPT_AMT'] == 11000)\n",
      "<ipython-input-126-e7011a4bfa49>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_ACUM_RCPT_AMT_49900\"] = (df_train['ACUM_RCPT_AMT'] == 49900)\n",
      "<ipython-input-126-e7011a4bfa49>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_ACUM_RCPT_AMT_49900\"] = (df_test['ACUM_RCPT_AMT'] == 49900)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"is_ACUM_RCPT_AMT_0\"] = (df_train['ACUM_RCPT_AMT'] == 0)\n",
    "df_test[\"is_ACUM_RCPT_AMT_0\"] = (df_test['ACUM_RCPT_AMT'] == 0)\n",
    "df_train[\"is_ACUM_RCPT_AMT_11000\"] = (df_train['ACUM_RCPT_AMT'] == 11000)\n",
    "df_test[\"is_ACUM_RCPT_AMT_11000\"] = (df_test['ACUM_RCPT_AMT'] == 11000)\n",
    "df_train[\"is_ACUM_RCPT_AMT_49900\"] = (df_train['ACUM_RCPT_AMT'] == 49900)\n",
    "df_test[\"is_ACUM_RCPT_AMT_49900\"] = (df_test['ACUM_RCPT_AMT'] == 49900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'is_ACUM_RCPT_AMT_0', 'is_ACUM_RCPT_AMT_11000', 'is_ACUM_RCPT_AMT_49900']]\n",
    "df_test = df_test[['id', 'is_ACUM_RCPT_AMT_0', 'is_ACUM_RCPT_AMT_11000', 'is_ACUM_RCPT_AMT_49900']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CP_M_CLF_NM  == 게임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"bin_CP_M_CLF_NM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'CP_M_CLF_NM']]\n",
    "df_test = test_df[['id', 'CP_M_CLF_NM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "donate_num = decoder['CP_M_CLF_NM']['게임']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-132-5b3843cdf182>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_CP_M_CLF_NM_game\"] = (df_train['CP_M_CLF_NM'] == donate_num)\n",
      "<ipython-input-132-5b3843cdf182>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_CP_M_CLF_NM_game\"] = (df_test['CP_M_CLF_NM'] == donate_num)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"is_CP_M_CLF_NM_game\"] = (df_train['CP_M_CLF_NM'] == donate_num)\n",
    "df_test[\"is_CP_M_CLF_NM_game\"] = (df_test['CP_M_CLF_NM'] == donate_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'is_CP_M_CLF_NM_game']]\n",
    "df_test = df_test[['id', 'is_CP_M_CLF_NM_game']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CP_S_CLF_NM == 게임 / 교통카드 충전 / 음악으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"bin_CP_S_CLF_NM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'CP_S_CLF_NM']]\n",
    "df_test = test_df[['id', 'CP_S_CLF_NM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_num = decoder['CP_S_CLF_NM']['게임']\n",
    "bus_num = decoder['CP_S_CLF_NM']['교통카드 충전']\n",
    "music_num = decoder['CP_S_CLF_NM']['음악']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-139-12cafddd5742>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_CP_S_CLF_NM_game\"] = (df_train['CP_S_CLF_NM'] == game_num)\n",
      "<ipython-input-139-12cafddd5742>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_CP_S_CLF_NM_game\"] = (df_test['CP_S_CLF_NM'] == game_num)\n",
      "<ipython-input-139-12cafddd5742>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_CP_S_CLF_NM_bus\"] = (df_train['CP_S_CLF_NM'] == bus_num)\n",
      "<ipython-input-139-12cafddd5742>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_CP_S_CLF_NM_bus\"] = (df_test['CP_S_CLF_NM'] == bus_num)\n",
      "<ipython-input-139-12cafddd5742>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_CP_S_CLF_NM_music\"] = (df_train['CP_S_CLF_NM'] == music_num)\n",
      "<ipython-input-139-12cafddd5742>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_CP_S_CLF_NM_music\"] = (df_test['CP_S_CLF_NM'] == music_num)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"is_CP_S_CLF_NM_game\"] = (df_train['CP_S_CLF_NM'] == game_num)\n",
    "df_test[\"is_CP_S_CLF_NM_game\"] = (df_test['CP_S_CLF_NM'] == game_num)\n",
    "df_train[\"is_CP_S_CLF_NM_bus\"] = (df_train['CP_S_CLF_NM'] == bus_num)\n",
    "df_test[\"is_CP_S_CLF_NM_bus\"] = (df_test['CP_S_CLF_NM'] == bus_num)\n",
    "df_train[\"is_CP_S_CLF_NM_music\"] = (df_train['CP_S_CLF_NM'] == music_num)\n",
    "df_test[\"is_CP_S_CLF_NM_music\"] = (df_test['CP_S_CLF_NM'] == music_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'is_CP_S_CLF_NM_game', 'is_CP_S_CLF_NM_bus', 'is_CP_S_CLF_NM_music']]\n",
    "df_test = df_test[['id', 'is_CP_S_CLF_NM_game', 'is_CP_S_CLF_NM_bus', 'is_CP_S_CLF_NM_music']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPAY_AMT_24M > 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"bin_NPAY_AMT_24M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'NPAY_AMT_24M']]\n",
    "df_test = test_df[['id', 'NPAY_AMT_24M']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-144-288b4392839b>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"NPAY_AMT_24M_over_40000\"] = (df_train['NPAY_AMT_24M'] > 40000)\n",
      "<ipython-input-144-288b4392839b>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"NPAY_AMT_24M_over_40000\"] = (df_test['NPAY_AMT_24M'] > 40000)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"NPAY_AMT_24M_over_40000\"] = (df_train['NPAY_AMT_24M'] > 40000)\n",
    "df_test[\"NPAY_AMT_24M_over_40000\"] = (df_test['NPAY_AMT_24M'] > 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'NPAY_AMT_24M_over_40000']]\n",
    "df_test = df_test[['id', 'NPAY_AMT_24M_over_40000']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVG_AMT_6M > 33000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"bin_AVG_AMT_6M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'AVG_AMT_6M']]\n",
    "df_test = test_df[['id', 'AVG_AMT_6M']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-149-5eeeb77ea9fa>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"AVG_AMT_6M_over_33000\"] = (df_train['AVG_AMT_6M'] > 33000)\n",
      "<ipython-input-149-5eeeb77ea9fa>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"AVG_AMT_6M_over_33000\"] = (df_test['AVG_AMT_6M'] > 33000)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"AVG_AMT_6M_over_33000\"] = (df_train['AVG_AMT_6M'] > 33000)\n",
    "df_test[\"AVG_AMT_6M_over_33000\"] = (df_test['AVG_AMT_6M'] > 33000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'AVG_AMT_6M_over_33000']]\n",
    "df_test = df_test[['id', 'AVG_AMT_6M_over_33000']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAX_LMT_3M_RT == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"bin_MAX_LMT_3M_RT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'MAX_LMT_3M_RT']]\n",
    "df_test = test_df[['id', 'MAX_LMT_3M_RT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-154-90ca19fa3a99>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_MAX_LMT_3M_RT_1\"] = (df_train['MAX_LMT_3M_RT'] == 1)\n",
      "<ipython-input-154-90ca19fa3a99>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_MAX_LMT_3M_RT_1\"] = (df_test['MAX_LMT_3M_RT'] == 1)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"is_MAX_LMT_3M_RT_1\"] = (df_train['MAX_LMT_3M_RT'] == 1)\n",
    "df_test[\"is_MAX_LMT_3M_RT_1\"] = (df_test['MAX_LMT_3M_RT'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'is_MAX_LMT_3M_RT_1']]\n",
    "df_test = df_test[['id', 'is_MAX_LMT_3M_RT_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPAY_CNT_24M == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"bin_NPAY_CNT_24M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'NPAY_CNT_24M']]\n",
    "df_test = test_df[['id', 'NPAY_CNT_24M']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-159-1e8b35d3d686>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_NPAY_CNT_24M_0\"] = (df_train['NPAY_CNT_24M'] == 0)\n",
      "<ipython-input-159-1e8b35d3d686>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_NPAY_CNT_24M_0\"] = (df_test['NPAY_CNT_24M'] == 0)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"is_NPAY_CNT_24M_0\"] = (df_train['NPAY_CNT_24M'] == 0)\n",
    "df_test[\"is_NPAY_CNT_24M_0\"] = (df_test['NPAY_CNT_24M'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'is_NPAY_CNT_24M_0']]\n",
    "df_test = df_test[['id', 'is_NPAY_CNT_24M_0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPAY_AMT_60M >= 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"bin_NPAY_AMT_60M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'NPAY_AMT_60M']]\n",
    "df_test = test_df[['id', 'NPAY_AMT_60M']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-164-334123a4b07d>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_NPAY_AMT_60M_over_40000\"] = (df_train['NPAY_AMT_60M'] > 40000)\n",
      "<ipython-input-164-334123a4b07d>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_NPAY_AMT_60M_over_40000\"] = (df_test['NPAY_AMT_60M'] > 40000)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"is_NPAY_AMT_60M_over_40000\"] = (df_train['NPAY_AMT_60M'] > 40000)\n",
    "df_test[\"is_NPAY_AMT_60M_over_40000\"] = (df_test['NPAY_AMT_60M'] > 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'is_NPAY_AMT_60M_over_40000']]\n",
    "df_test = df_test[['id', 'is_NPAY_AMT_60M_over_40000']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp data/task150/backup_11140751/* data/task150/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 고객별 거래금액 AC_PAY_AMT sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 11:55:09.286570\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"sum_pay_wrt_seq\"\n",
    "COLUMN = \"PAYR_SEQ\"\n",
    "TARGET_LIST = [\"AC_PAY_AMT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST], test_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST]], \n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: AC_PAY_AMT\n",
      "window: 10, target: AC_PAY_AMT\n",
      "window: 30, target: AC_PAY_AMT\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}sum_{target}_wrt_{COLUMN}_{window}\"\n",
    "        if path.exists(data_path):\n",
    "            print(f\"{data_path} exists!\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        temp_df = df[[COLUMN, target, 'REQ_DD']].sort_values(by=['REQ_DD'])\n",
    "        temp_df['datetime'] = pd.to_datetime(temp_df['REQ_DD'], format='%Y%m%d')\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "        \n",
    "        sum_list = []\n",
    "        start_date = \"2019-07-01\"\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        while True:\n",
    "            end_date = start_date + datetime.timedelta(days=window)\n",
    "\n",
    "            if end_date > datetime.datetime.strptime(\"2019-12-31\", \"%Y-%m-%d\"):\n",
    "                break\n",
    "\n",
    "            sum_df = temp_df[(temp_df['datetime']>=start_date)&(temp_df['datetime']<end_date)][[COLUMN, target]]\n",
    "            sum_df = sum_df.groupby([COLUMN]).sum().reset_index()\n",
    "            sum_df.columns = [COLUMN, f'sum_{target}_wrt_{COLUMN}_{window}']\n",
    "            sum_df['REQ_DD'] = datetime.datetime.strftime(end_date, \"%Y%m%d\")\n",
    "\n",
    "            sum_list.append(sum_df)\n",
    "            start_date = start_date + datetime.timedelta(days=1)\n",
    "        sum_df = pd.concat(sum_list, axis=0)\n",
    "        sum_df.to_parquet(data_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})\n",
    "df_test = test_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: AC_PAY_AMT\n",
      "window: 10, target: AC_PAY_AMT\n",
      "window: 30, target: AC_PAY_AMT\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}sum_{target}_wrt_{COLUMN}_{window}\"\n",
    "        \n",
    "        sum_df = pd.read_parquet(data_path).astype({'REQ_DD': 'int32'})\n",
    "        df_train = df_train.merge(sum_df, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "        df_test = df_test.merge(sum_df, on=[COLUMN, 'REQ_DD'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].drop(columns=[COLUMN, 'REQ_DD']).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(columns=[COLUMN, 'REQ_DD']).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-14 12:01:29.325313\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 휴대폰별 거래금액 AC_PAY_AMT sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 12:30:01.533477\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"sum_pay_wrt_phone\"\n",
    "COLUMN = \"MPHN_NO\"\n",
    "TARGET_LIST = [\"AC_PAY_AMT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST], test_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST]], \n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: AC_PAY_AMT\n",
      "window: 10, target: AC_PAY_AMT\n",
      "window: 30, target: AC_PAY_AMT\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}sum_{target}_wrt_{COLUMN}_{window}\"\n",
    "        if path.exists(data_path):\n",
    "            print(f\"{data_path} exists!\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        temp_df = df[[COLUMN, target, 'REQ_DD']].sort_values(by=['REQ_DD'])\n",
    "        temp_df['datetime'] = pd.to_datetime(temp_df['REQ_DD'], format='%Y%m%d')\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "        \n",
    "        sum_list = []\n",
    "        start_date = \"2019-07-01\"\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        while True:\n",
    "            end_date = start_date + datetime.timedelta(days=window)\n",
    "\n",
    "            if end_date > datetime.datetime.strptime(\"2019-12-31\", \"%Y-%m-%d\"):\n",
    "                break\n",
    "\n",
    "            sum_df = temp_df[(temp_df['datetime']>=start_date)&(temp_df['datetime']<end_date)][[COLUMN, target]]\n",
    "            sum_df = sum_df.groupby([COLUMN]).sum().reset_index()\n",
    "            sum_df.columns = [COLUMN, f'sum_{target}_wrt_{COLUMN}_{window}']\n",
    "            sum_df['REQ_DD'] = datetime.datetime.strftime(end_date, \"%Y%m%d\")\n",
    "\n",
    "            sum_list.append(sum_df)\n",
    "            start_date = start_date + datetime.timedelta(days=1)\n",
    "        sum_df = pd.concat(sum_list, axis=0)\n",
    "        sum_df.to_parquet(data_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: AC_PAY_AMT\n",
      "window: 10, target: AC_PAY_AMT\n",
      "window: 30, target: AC_PAY_AMT\n"
     ]
    }
   ],
   "source": [
    "df_train = train_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})\n",
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}sum_{target}_wrt_{COLUMN}_{window}\"\n",
    "        \n",
    "        sum_df = pd.read_parquet(data_path).astype({'REQ_DD': 'int32'})\n",
    "        df_train = df_train.merge(sum_df, on=[COLUMN, 'REQ_DD'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].drop(columns=[COLUMN, 'REQ_DD']).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function gc.collect(generation=2)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_train\n",
    "gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: AC_PAY_AMT\n",
      "window: 10, target: AC_PAY_AMT\n",
      "window: 30, target: AC_PAY_AMT\n"
     ]
    }
   ],
   "source": [
    "df_test = test_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})\n",
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}sum_{target}_wrt_{COLUMN}_{window}\"\n",
    "        \n",
    "        sum_df = pd.read_parquet(data_path).astype({'REQ_DD': 'int32'})\n",
    "        df_test = df_test.merge(sum_df, on=[COLUMN, 'REQ_DD'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(columns=[COLUMN, 'REQ_DD']).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-14 12:37:38.436752\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 최근 미납 횟수 NPAY_YN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-14 12:12:45.995999\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = [3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"npay_wrt_seq\"\n",
    "COLUMN = \"PAYR_SEQ\"\n",
    "TARGET_LIST = [\"NPAY_YN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST], test_df[['id', COLUMN, 'REQ_DD'] + TARGET_LIST]], \n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[COLUMN] = np.where((df[COLUMN]==1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: NPAY_YN\n",
      "window: 10, target: NPAY_YN\n",
      "window: 30, target: NPAY_YN\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}sum_{target}_wrt_{COLUMN}_{window}\"\n",
    "        if path.exists(data_path):\n",
    "            print(f\"{data_path} exists!\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        temp_df = df[[COLUMN, target, 'REQ_DD']].sort_values(by=['REQ_DD'])\n",
    "        temp_df['datetime'] = pd.to_datetime(temp_df['REQ_DD'], format='%Y%m%d')\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "        \n",
    "        sum_list = []\n",
    "        start_date = \"2019-07-01\"\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        while True:\n",
    "            end_date = start_date + datetime.timedelta(days=window)\n",
    "\n",
    "            if end_date > datetime.datetime.strptime(\"2019-12-31\", \"%Y-%m-%d\"):\n",
    "                break\n",
    "\n",
    "            sum_df = temp_df[(temp_df['datetime']>=start_date)&(temp_df['datetime']<end_date)][[COLUMN, target]]\n",
    "            sum_df = sum_df.groupby([COLUMN]).sum().reset_index()\n",
    "            sum_df.columns = [COLUMN, f'sum_{target}_wrt_{COLUMN}_{window}']\n",
    "            sum_df['REQ_DD'] = datetime.datetime.strftime(end_date, \"%Y%m%d\")\n",
    "\n",
    "            sum_list.append(sum_df)\n",
    "            start_date = start_date + datetime.timedelta(days=1)\n",
    "        sum_df = pd.concat(sum_list, axis=0)\n",
    "        sum_df.to_parquet(data_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})\n",
    "df_test = test_df[['id', 'REQ_DD', COLUMN]].astype({'REQ_DD': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 3, target: NPAY_YN\n",
      "window: 10, target: NPAY_YN\n",
      "window: 30, target: NPAY_YN\n"
     ]
    }
   ],
   "source": [
    "for window in WINDOW_SIZE:\n",
    "    for target in TARGET_LIST:\n",
    "        print(f\"window: {window}, target: {target}\")\n",
    "        data_path = f\"{data_dir}sum_{target}_wrt_{COLUMN}_{window}\"\n",
    "        \n",
    "        sum_df = pd.read_parquet(data_path).astype({'REQ_DD': 'int32'})\n",
    "        df_train = df_train.merge(sum_df, on=[COLUMN, 'REQ_DD'], how='left')\n",
    "        df_test = df_test.merge(sum_df, on=[COLUMN, 'REQ_DD'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].drop(columns=[COLUMN, 'REQ_DD']).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(columns=[COLUMN, 'REQ_DD']).fillna(0).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-14 12:17:04.043575\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
